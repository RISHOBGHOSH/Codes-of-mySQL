{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1YvOzQM2gLfd6AD5kHzT1VoQxIR0oEm8P",
      "authorship_tag": "ABX9TyOuQGklf3jIve+rjpDMSgCy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RISHOBGHOSH/Codes-of-mySQL/blob/main/spark_UI_Z2H.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"Joins and Data Partitions\")\n",
        "    .master(\"local[*]\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "X4VsthGjNL3Y",
        "outputId": "bcc0a8c2-3fb4-4c3e-8ec8-58ebd025b3b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7cc3cb15fd70>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e540ebdaa4bb:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Joins and Data Partitions</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Emp Data & Schema\n",
        "\n",
        "emp_data = [\n",
        "    [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"],\n",
        "    [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"],\n",
        "    [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"],\n",
        "    [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"],\n",
        "    [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"],\n",
        "    [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"],\n",
        "    [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"],\n",
        "    [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"],\n",
        "    [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"],\n",
        "    [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"],\n",
        "    [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"],\n",
        "    [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"],\n",
        "    [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"],\n",
        "    [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"],\n",
        "    [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"],\n",
        "    [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"],\n",
        "    [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"],\n",
        "    [\"018\",\"104\",\"Nancy Liu\",\"29\",\"\",\"50000\",\"2017-06-01\"],\n",
        "    [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"],\n",
        "    [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"]\n",
        "]\n",
        "\n",
        "emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\"\n",
        "\n",
        "dept_data = [\n",
        "    [\"101\", \"Sales\", \"NYC\", \"US\", \"1000000\"],\n",
        "    [\"102\", \"Marketing\", \"LA\", \"US\", \"900000\"],\n",
        "    [\"103\", \"Finance\", \"London\", \"UK\", \"1200000\"],\n",
        "    [\"104\", \"Engineering\", \"Beijing\", \"China\", \"1500000\"],\n",
        "    [\"105\", \"Human Resources\", \"Tokyo\", \"Japan\", \"800000\"],\n",
        "    [\"106\", \"Research and Development\", \"Perth\", \"Australia\", \"1100000\"],\n",
        "    [\"107\", \"Customer Service\", \"Sydney\", \"Australia\", \"950000\"]\n",
        "]\n",
        "\n",
        "dept_schema = \"department_id string, department_name string, city string, country string, budget string\""
      ],
      "metadata": {
        "id": "Cnz-nOHqNRPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create emp & dept DataFrame\n",
        "\n",
        "emp = spark.createDataFrame(data=emp_data, schema=emp_schema)\n",
        "dept = spark.createDataFrame(data=dept_data, schema=dept_schema)"
      ],
      "metadata": {
        "id": "YLOhm_P-NYuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show emp dataframe (ACTION)\n",
        "\n",
        "emp.show()\n",
        "dept.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya_Pc2oggVe8",
        "outputId": "ebc8f4e8-23f5-4a91-8d6f-9ddcc2703fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
            "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
            "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
            "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
            "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
            "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
            "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
            "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
            "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
            "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
            "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
            "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
            "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
            "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
            "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|\n",
            "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
            "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "\n",
            "+-------------+--------------------+-------+---------+-------+\n",
            "|department_id|     department_name|   city|  country| budget|\n",
            "+-------------+--------------------+-------+---------+-------+\n",
            "|          101|               Sales|    NYC|       US|1000000|\n",
            "|          102|           Marketing|     LA|       US| 900000|\n",
            "|          103|             Finance| London|       UK|1200000|\n",
            "|          104|         Engineering|Beijing|    China|1500000|\n",
            "|          105|     Human Resources|  Tokyo|    Japan| 800000|\n",
            "|          106|Research and Deve...|  Perth|Australia|1100000|\n",
            "|          107|    Customer Service| Sydney|Australia| 950000|\n",
            "+-------------+--------------------+-------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Schema\n",
        "\n",
        "emp.printSchema()\n",
        "dept.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKt_m3mRgbxQ",
        "outputId": "f6e73592-e652-4553-ab95-bdb1c63fa35e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_id: string (nullable = true)\n",
            " |-- department_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: string (nullable = true)\n",
            " |-- hire_date: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- department_id: string (nullable = true)\n",
            " |-- department_name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- budget: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get number of partitions for emp\n",
        "emp.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Od2w4v8ggT1",
        "outputId": "0dc83d75-13c4-4d2e-dd15-370b862f5e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get number of partitions for dept\n",
        "dept.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gz1dHLPgnAc",
        "outputId": "9bd153bf-f998-4792-9e1a-efae3aab1ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Repartition of data using repartition & coalesce\n",
        "emp_partitioned = emp.repartition(4)"
      ],
      "metadata": {
        "id": "41_Ex7SPgx-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emp_partitioned.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEeMcLRXkjY1",
        "outputId": "f7150720-5c8b-49ad-ba11-cce30361155e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can increase the partition\n",
        "emp_partitioned = emp.repartition(10)\n",
        "emp_partitioned.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnsJj1DElIp6",
        "outputId": "476af5e9-b28b-4fe0-8ca7-43b3a8a59b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⭐ 1. coalesce()\n",
        "✔ Used to reduce the number of partitions\n",
        "\n",
        "(e.g., from 20 → 5)\n",
        "\n",
        "✔ NO shuffle (narrow transformation)\n",
        "\n",
        "Faster because data doesn’t move across nodes.\n",
        "\n",
        "✔ Best when:\n",
        "\n",
        "Reducing partitions\n",
        "\n",
        "After filtering data (less data left)\n",
        "\n",
        "Before writing to fewer output files\n",
        "\n",
        "Improving performance by reducing overhead\n",
        "\n",
        "❌ Not good for increasing partitions\n",
        "\n",
        "Spark won’t redistribute data evenly."
      ],
      "metadata": {
        "id": "eCyEJJ9xl2kS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⭐ 2. repartition()\n",
        "✔ Used to increase OR decrease partitions\n",
        "\n",
        "(e.g., 10 → 50 or 50 → 10)\n",
        "\n",
        "✔ FULL SHUFFLE (wide transformation)\n",
        "\n",
        "Data is evenly redistributed across partitions.\n",
        "\n",
        "✔ Best when:\n",
        "\n",
        "Increasing partitions\n",
        "\n",
        "Preparing for heavy parallel processing\n",
        "\n",
        "Fixing skew\n",
        "\n",
        "Preparing for joins / aggregations\n",
        "\n",
        "Ensuring balanced partitions\n",
        "\n",
        "❌ Slower than coalesce (due to shuffle)"
      ],
      "metadata": {
        "id": "FltoTpgWl5n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_partitioned = emp.coalesce(200)\n",
        "emp_partitioned.rdd.getNumPartitions()\n",
        "# coalesce cant increase the num of partition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5wCHf3knGdo",
        "outputId": "306f8b7a-2729-4d66-f427-f66aee24ade0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_partitioned = emp.coalesce(1)\n",
        "emp_partitioned.rdd.getNumPartitions()\n",
        "# coalesce cant increase the num of partition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTP-uq_fneVG",
        "outputId": "8f7e89bb-2067-4718-f53b-1334842892cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Repartition of data using repartition & coalesce for one column\n",
        "emp_partitioned = emp.repartition(4, \"department_id\")"
      ],
      "metadata": {
        "id": "yKdedemBkr0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the partition info for partitions and reparition\n",
        "from pyspark.sql.functions import spark_partition_id\n",
        "\n",
        "emp_1 = emp.withColumn(\"partition_num\", spark_partition_id())\n",
        "emp_1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IVMSQ6yojEF",
        "outputId": "b9c51af6-1a63-4844-91b6-f4b98d84a6cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+-------------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|partition_num|\n",
            "+-----------+-------------+-------------+---+------+------+----------+-------------+\n",
            "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|            0|\n",
            "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|            0|\n",
            "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|            0|\n",
            "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|            0|\n",
            "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|            0|\n",
            "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|            0|\n",
            "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|            0|\n",
            "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|            0|\n",
            "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|            0|\n",
            "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|            0|\n",
            "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|            1|\n",
            "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|            1|\n",
            "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|            1|\n",
            "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|            1|\n",
            "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|            1|\n",
            "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|            1|\n",
            "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|            1|\n",
            "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|            1|\n",
            "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|            1|\n",
            "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|            1|\n",
            "+-----------+-------------+-------------+---+------+------+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the partition info for partitions and reparition\n",
        "from pyspark.sql.functions import spark_partition_id\n",
        "\n",
        "emp_1 = emp.repartition(4, \"department_id\").withColumn(\"partition_num\", spark_partition_id())\n",
        "emp_1.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRwu3_lToVRA",
        "outputId": "b47560d1-54ff-4e68-8fba-cb0e0d14a2a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+-------------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|partition_num|\n",
            "+-----------+-------------+-------------+---+------+------+----------+-------------+\n",
            "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|            0|\n",
            "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|            0|\n",
            "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|            0|\n",
            "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|            0|\n",
            "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|            0|\n",
            "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|            0|\n",
            "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|            1|\n",
            "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|            1|\n",
            "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|            2|\n",
            "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|            2|\n",
            "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|            2|\n",
            "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|            2|\n",
            "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|            2|\n",
            "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|            3|\n",
            "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|            3|\n",
            "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|            3|\n",
            "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|            3|\n",
            "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|            3|\n",
            "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|            3|\n",
            "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|            3|\n",
            "+-----------+-------------+-------------+---+------+------+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "sAYA-643us0x",
        "outputId": "f692cf69-5fa8-4799-8c7b-7d71b3969da1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78bf0ed364e0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://503b4c7f3ca9:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Reading from CSV Files</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"Reading from CSV Files\")\n",
        "    .master(\"local[*]\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read a csv file into dataframe\n",
        "\n",
        "# df = spark.read.format(\"csv\").load(\"data/input/emp.csv\")\n",
        "df = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"/content/emp.csv\")\n",
        "\n",
        "# here we have spark jobs and it will ran to process the metadata\n",
        "#option (header , true) - it will read the 7 coulmns\n",
        "#option(inferSchema, True) - will try to look at the files and understand the data type\n",
        "# spark - jobs , then stage and then task"
      ],
      "metadata": {
        "id": "resMk7TAxPmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2PE0Kd84y7Q",
        "outputId": "e8713610-4cd0-45cc-c80f-1af9cd81a9b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_id: integer (nullable = true)\n",
            " |-- department_id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            " |-- hire_date: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "8q3WCmlTxqk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de73608-7830-4b6c-a23f-97d83b07ce59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
            "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
            "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
            "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
            "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
            "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
            "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
            "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
            "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
            "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
            "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
            "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
            "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
            "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
            "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
            "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
            "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading with Schema\n",
        "_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date\"\n",
        "\n",
        "df_schema = spark.read.format(\"csv\").schema(_schema).load(\"/content/emp.csv\")\n",
        "\n",
        "# here the spark did not created any Jobs to read the schema\n",
        "# becoz we already gave the schema\n",
        "# so in production we have schema - and to optimize the code\n",
        "# becoz there ight me prb while spark reading the file which has data from different sources\n"
      ],
      "metadata": {
        "id": "HjgyicjQxX3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_schema.show() # NUll in first line as we have not specified the header\n",
        "\n",
        "df_schema1 = spark.read.format(\"csv\").option(\"header\",True).schema(_schema).load(\"/content/emp.csv\")\n",
        "df_schema1.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSr7bN8o6421",
        "outputId": "003f9447-63ff-48ce-f04e-30e636b0b43f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+----+------+-------+----------+\n",
            "|employee_id|department_id|         name| age|gender| salary| hire_date|\n",
            "+-----------+-------------+-------------+----+------+-------+----------+\n",
            "|       NULL|         NULL|         name|NULL|gender|   NULL|      NULL|\n",
            "|          1|          101|     John Doe|  30|  Male|50000.0|2015-01-01|\n",
            "|          2|          101|   Jane Smith|  25|Female|45000.0|2016-02-15|\n",
            "|          3|          102|    Bob Brown|  35|  Male|55000.0|2014-05-01|\n",
            "|          4|          102|    Alice Lee|  28|Female|48000.0|2017-09-30|\n",
            "|          5|          103|    Jack Chan|  40|  Male|60000.0|2013-04-01|\n",
            "|          6|          103|    Jill Wong|  32|Female|52000.0|2018-07-01|\n",
            "|          7|          101|James Johnson|  42|  Male|70000.0|2012-03-15|\n",
            "|          8|          102|     Kate Kim|  29|Female|51000.0|2019-10-01|\n",
            "|          9|          103|      Tom Tan|  33|  Male|58000.0|2016-06-01|\n",
            "|         10|          104|     Lisa Lee|  27|Female|47000.0|2018-08-01|\n",
            "|         11|          104|   David Park|  38|  Male|65000.0|2015-11-01|\n",
            "|         12|          105|   Susan Chen|  31|Female|54000.0|2017-02-15|\n",
            "|         13|          106|    Brian Kim|  45|  Male|75000.0|2011-07-01|\n",
            "|         14|          107|    Emily Lee|  26|Female|46000.0|2019-01-01|\n",
            "|         15|          106|  Michael Lee|  37|  Male|63000.0|2014-09-30|\n",
            "|         16|          107|  Kelly Zhang|  30|Female|49000.0|2018-04-01|\n",
            "|         17|          105|  George Wang|  34|  Male|57000.0|2016-03-15|\n",
            "|         18|          104|    Nancy Liu|  29|Female|50000.0|2017-06-01|\n",
            "|         19|          103|  Steven Chen|  36|  Male|62000.0|2015-08-01|\n",
            "+-----------+-------------+-------------+----+------+-------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----------+-------------+-------------+---+------+-------+----------+\n",
            "|employee_id|department_id|         name|age|gender| salary| hire_date|\n",
            "+-----------+-------------+-------------+---+------+-------+----------+\n",
            "|          1|          101|     John Doe| 30|  Male|50000.0|2015-01-01|\n",
            "|          2|          101|   Jane Smith| 25|Female|45000.0|2016-02-15|\n",
            "|          3|          102|    Bob Brown| 35|  Male|55000.0|2014-05-01|\n",
            "|          4|          102|    Alice Lee| 28|Female|48000.0|2017-09-30|\n",
            "|          5|          103|    Jack Chan| 40|  Male|60000.0|2013-04-01|\n",
            "|          6|          103|    Jill Wong| 32|Female|52000.0|2018-07-01|\n",
            "|          7|          101|James Johnson| 42|  Male|70000.0|2012-03-15|\n",
            "|          8|          102|     Kate Kim| 29|Female|51000.0|2019-10-01|\n",
            "|          9|          103|      Tom Tan| 33|  Male|58000.0|2016-06-01|\n",
            "|         10|          104|     Lisa Lee| 27|Female|47000.0|2018-08-01|\n",
            "|         11|          104|   David Park| 38|  Male|65000.0|2015-11-01|\n",
            "|         12|          105|   Susan Chen| 31|Female|54000.0|2017-02-15|\n",
            "|         13|          106|    Brian Kim| 45|  Male|75000.0|2011-07-01|\n",
            "|         14|          107|    Emily Lee| 26|Female|46000.0|2019-01-01|\n",
            "|         15|          106|  Michael Lee| 37|  Male|63000.0|2014-09-30|\n",
            "|         16|          107|  Kelly Zhang| 30|Female|49000.0|2018-04-01|\n",
            "|         17|          105|  George Wang| 34|  Male|57000.0|2016-03-15|\n",
            "|         18|          104|    Nancy Liu| 29|Female|50000.0|2017-06-01|\n",
            "|         19|          103|  Steven Chen| 36|  Male|62000.0|2015-08-01|\n",
            "|         20|          102|    Grace Kim| 32|Female|53000.0|2018-11-01|\n",
            "+-----------+-------------+-------------+---+------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle BAD records - PERMISSIVE (Default mode)\n",
        "_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date ,_corrupt_record string\"\n",
        "#add _corrupt_record string is see the Data which are BAD\n",
        "df_p = spark.read.format(\"csv\").schema(_schema).load(\"/content/emp_new.csv\")\n",
        "df_p.printSchema()\n",
        "df_p.show()\n",
        "\n",
        "# shows the columns with BAD data\n",
        "df_p.where(\"_corrupt_record is not null\").show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUtdD4rb8Fs6",
        "outputId": "045d25f2-f41e-400f-8d15-3ad24149a254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_id: integer (nullable = true)\n",
            " |-- department_id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            " |-- hire_date: date (nullable = true)\n",
            " |-- _corrupt_record: string (nullable = true)\n",
            "\n",
            "+-----------+-------------+-------------+----+------+-------+----------+--------------------+\n",
            "|employee_id|department_id|         name| age|gender| salary| hire_date|     _corrupt_record|\n",
            "+-----------+-------------+-------------+----+------+-------+----------+--------------------+\n",
            "|       NULL|         NULL|         name|NULL|gender|   NULL|      NULL|employee_id,depar...|\n",
            "|          1|          101|     John Doe|  30|  Male|50000.0|2015-01-01|                NULL|\n",
            "|          2|          101|   Jane Smith|  25|Female|45000.0|2016-02-15|                NULL|\n",
            "|          3|          102|    Bob Brown|  35|  Male|55000.0|2014-05-01|                NULL|\n",
            "|          4|          102|    Alice Lee|  28|Female|48000.0|2017-09-30|                NULL|\n",
            "|          5|          103|    Jack Chan|  40|  Male|60000.0|2013-04-01|                NULL|\n",
            "|          6|          103|    Jill Wong|  32|Female|52000.0|2018-07-01|                NULL|\n",
            "|          7|          101|James Johnson|  42|  Male|   NULL|2012-03-15|007,101,James Joh...|\n",
            "|          8|          102|     Kate Kim|  29|Female|51000.0|2019-10-01|                NULL|\n",
            "|          9|          103|      Tom Tan|  33|  Male|58000.0|2016-06-01|                NULL|\n",
            "|         10|          104|     Lisa Lee|  27|Female|47000.0|2018-08-01|                NULL|\n",
            "|         11|          104|   David Park|  38|  Male|65000.0|      NULL|011,104,David Par...|\n",
            "|         12|          105|   Susan Chen|  31|Female|54000.0|2017-02-15|                NULL|\n",
            "|         13|          106|    Brian Kim|  45|  Male|75000.0|2011-07-01|                NULL|\n",
            "|         14|          107|    Emily Lee|  26|Female|46000.0|2019-01-01|                NULL|\n",
            "|         15|          106|  Michael Lee|  37|  Male|63000.0|2014-09-30|                NULL|\n",
            "|         16|          107|  Kelly Zhang|  30|Female|49000.0|2018-04-01|                NULL|\n",
            "|         17|          105|  George Wang|  34|  Male|57000.0|2016-03-15|                NULL|\n",
            "|         18|          104|    Nancy Liu|  29|Female|50000.0|2017-06-01|                NULL|\n",
            "|         19|          103|  Steven Chen|  36|  Male|62000.0|2015-08-01|                NULL|\n",
            "+-----------+-------------+-------------+----+------+-------+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----------+-------------+-------------+----+------+-------+----------+--------------------+\n",
            "|employee_id|department_id|         name| age|gender| salary| hire_date|     _corrupt_record|\n",
            "+-----------+-------------+-------------+----+------+-------+----------+--------------------+\n",
            "|       NULL|         NULL|         name|NULL|gender|   NULL|      NULL|employee_id,depar...|\n",
            "|          7|          101|James Johnson|  42|  Male|   NULL|2012-03-15|007,101,James Joh...|\n",
            "|         11|          104|   David Park|  38|  Male|65000.0|      NULL|011,104,David Par...|\n",
            "+-----------+-------------+-------------+----+------+-------+----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#columnNameOfCorruptRecord\n",
        "_schema1 = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date, bad_record string\"\n",
        "\n",
        "df_p1 = spark.read.format(\"csv\").schema(_schema1).option(\"columnNameOfCorruptRecord\", \"bad_record\").option(\"header\", True).load(\"/content/emp_new.csv\")\n",
        "df_p1.printSchema()\n",
        "df_p1.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkEC39-uDfXO",
        "outputId": "8051057d-c242-440a-b9a0-73353b9851f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_id: integer (nullable = true)\n",
            " |-- department_id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            " |-- hire_date: date (nullable = true)\n",
            " |-- bad_record: string (nullable = true)\n",
            "\n",
            "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
            "|employee_id|department_id|         name|age|gender| salary| hire_date|          bad_record|\n",
            "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
            "|          1|          101|     John Doe| 30|  Male|50000.0|2015-01-01|                NULL|\n",
            "|          2|          101|   Jane Smith| 25|Female|45000.0|2016-02-15|                NULL|\n",
            "|          3|          102|    Bob Brown| 35|  Male|55000.0|2014-05-01|                NULL|\n",
            "|          4|          102|    Alice Lee| 28|Female|48000.0|2017-09-30|                NULL|\n",
            "|          5|          103|    Jack Chan| 40|  Male|60000.0|2013-04-01|                NULL|\n",
            "|          6|          103|    Jill Wong| 32|Female|52000.0|2018-07-01|                NULL|\n",
            "|          7|          101|James Johnson| 42|  Male|   NULL|2012-03-15|007,101,James Joh...|\n",
            "|          8|          102|     Kate Kim| 29|Female|51000.0|2019-10-01|                NULL|\n",
            "|          9|          103|      Tom Tan| 33|  Male|58000.0|2016-06-01|                NULL|\n",
            "|         10|          104|     Lisa Lee| 27|Female|47000.0|2018-08-01|                NULL|\n",
            "|         11|          104|   David Park| 38|  Male|65000.0|      NULL|011,104,David Par...|\n",
            "|         12|          105|   Susan Chen| 31|Female|54000.0|2017-02-15|                NULL|\n",
            "|         13|          106|    Brian Kim| 45|  Male|75000.0|2011-07-01|                NULL|\n",
            "|         14|          107|    Emily Lee| 26|Female|46000.0|2019-01-01|                NULL|\n",
            "|         15|          106|  Michael Lee| 37|  Male|63000.0|2014-09-30|                NULL|\n",
            "|         16|          107|  Kelly Zhang| 30|Female|49000.0|2018-04-01|                NULL|\n",
            "|         17|          105|  George Wang| 34|  Male|57000.0|2016-03-15|                NULL|\n",
            "|         18|          104|    Nancy Liu| 29|Female|50000.0|2017-06-01|                NULL|\n",
            "|         19|          103|  Steven Chen| 36|  Male|62000.0|2015-08-01|                NULL|\n",
            "|         20|          102|    Grace Kim| 32|Female|53000.0|2018-11-01|                NULL|\n",
            "+-----------+-------------+-------------+---+------+-------+----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle BAD records - DROPMALFORMED\n",
        "_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date\"\n",
        "\n",
        "df_m = spark.read.format(\"csv\").option(\"header\", True).option(\"mode\", \"DROPMALFORMED\").schema(_schema).load(\"/content/emp_new.csv\")\n",
        "df_m.printSchema()\n",
        "\n",
        "df_m.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlIZv51FEQnR",
        "outputId": "fcd9fc01-06bf-4d17-919f-2cda98d98478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_id: integer (nullable = true)\n",
            " |-- department_id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            " |-- hire_date: date (nullable = true)\n",
            "\n",
            "+-----------+-------------+-----------+---+------+-------+----------+\n",
            "|employee_id|department_id|       name|age|gender| salary| hire_date|\n",
            "+-----------+-------------+-----------+---+------+-------+----------+\n",
            "|          1|          101|   John Doe| 30|  Male|50000.0|2015-01-01|\n",
            "|          2|          101| Jane Smith| 25|Female|45000.0|2016-02-15|\n",
            "|          3|          102|  Bob Brown| 35|  Male|55000.0|2014-05-01|\n",
            "|          4|          102|  Alice Lee| 28|Female|48000.0|2017-09-30|\n",
            "|          5|          103|  Jack Chan| 40|  Male|60000.0|2013-04-01|\n",
            "|          6|          103|  Jill Wong| 32|Female|52000.0|2018-07-01|\n",
            "|          8|          102|   Kate Kim| 29|Female|51000.0|2019-10-01|\n",
            "|          9|          103|    Tom Tan| 33|  Male|58000.0|2016-06-01|\n",
            "|         10|          104|   Lisa Lee| 27|Female|47000.0|2018-08-01|\n",
            "|         12|          105| Susan Chen| 31|Female|54000.0|2017-02-15|\n",
            "|         13|          106|  Brian Kim| 45|  Male|75000.0|2011-07-01|\n",
            "|         14|          107|  Emily Lee| 26|Female|46000.0|2019-01-01|\n",
            "|         15|          106|Michael Lee| 37|  Male|63000.0|2014-09-30|\n",
            "|         16|          107|Kelly Zhang| 30|Female|49000.0|2018-04-01|\n",
            "|         17|          105|George Wang| 34|  Male|57000.0|2016-03-15|\n",
            "|         18|          104|  Nancy Liu| 29|Female|50000.0|2017-06-01|\n",
            "|         19|          103|Steven Chen| 36|  Male|62000.0|2015-08-01|\n",
            "|         20|          102|  Grace Kim| 32|Female|53000.0|2018-11-01|\n",
            "+-----------+-------------+-----------+---+------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle BAD records - FAILFAST\n",
        "\n",
        "_schema = \"employee_id int, department_id int, name string, age int, gender string, salary double, hire_date date\"\n",
        "\n",
        "df_m2 = spark.read.format(\"csv\").option(\"header\", True).option(\"mode\", \"FAILFAST\").schema(_schema).load(\"/content/emp_new.csv\")\n",
        "\n",
        "df_m2.printSchema()\n",
        "\n",
        "df_m2.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KVAuY9iALvnJ",
        "outputId": "5293a4e0-4533-4a21-a054-5eda84fa83d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_id: integer (nullable = true)\n",
            " |-- department_id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            " |-- hire_date: date (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o584.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 23) (503b4c7f3ca9 executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [7,101,James Johnson,42,Male,null,15414].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Low\"\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.base/java.lang.Double.parseDouble(Double.java:651)\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:207)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:203)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 29 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat jdk.internal.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [7,101,James Johnson,42,Male,null,15414].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Low\"\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.base/java.lang.Double.parseDouble(Double.java:651)\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:207)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:203)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 29 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3583512437.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf_m2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf_m2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o584.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 23) (503b4c7f3ca9 executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [7,101,James Johnson,42,Male,null,15414].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Low\"\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.base/java.lang.Double.parseDouble(Double.java:651)\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:207)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:203)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 29 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat jdk.internal.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [7,101,James Johnson,42,Male,null,15414].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Low\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Low\"\n\tat java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)\n\tat java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n\tat java.base/java.lang.Double.parseDouble(Double.java:651)\n\tat scala.collection.immutable.StringLike.toDouble(StringLike.scala:327)\n\tat scala.collection.immutable.StringLike.toDouble$(StringLike.scala:327)\n\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$12(UnivocityParser.scala:207)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$11(UnivocityParser.scala:203)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 29 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PERMISSIVE\n",
        "\n",
        "Keeps bad rows with _corrupt_record, best for data exploration and debugging.\n",
        "\n",
        "DROPMALFORMED\n",
        "\n",
        "Drops bad rows; best when only clean data is required.\n",
        "\n",
        "FAILFAST\n",
        "\n",
        "Stops on first error; best for strong validation in production."
      ],
      "metadata": {
        "id": "OpmRun85MGgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BONUS TIP\n",
        "# Multiple options\n",
        "\n",
        "_options = {\n",
        "    \"header\" : \"true\",\n",
        "    \"inferSchema\" : \"true\",\n",
        "    \"mode\" : \"PERMISSIVE\"\n",
        "}\n",
        "\n",
        "df = (spark.read.format(\"csv\").options(**_options).load(\"/content/emp_new.csv\"))\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq_Fa26CMfLC",
        "outputId": "965a0d04-3969-42cc-80ba-6dc497dc3f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
            "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
            "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
            "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
            "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|          7|          101|James Johnson| 42|  Male|   Low|2012-03-15|\n",
            "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
            "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
            "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
            "|         11|          104|   David Park| 38|  Male| 65000|   no date|\n",
            "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
            "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
            "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
            "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
            "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
            "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
            "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
            "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HdYyL6-nSrrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13 Read Complex File Formats | Parquet | ORC | Performance benefit of Parquet |Recursive File Lookup"
      ],
      "metadata": {
        "id": "RQyQY7M7XH-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"Reading Complex Data Formats\")\n",
        "    .master(\"local[*]\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "vgrKFkuruI91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "8eab9786-d6e7-4ff1-eff9-fc866c5ef196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7cc3cb15fd70>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e540ebdaa4bb:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Joins and Data Partitions</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Parquet Sales data\n",
        "\n",
        "df_parquet = spark.read.format(\"parquet\").load(\"/content/sales_data.parquet\")"
      ],
      "metadata": {
        "id": "uRiMqC5gP_rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_parquet.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpnORVN3QLmc",
        "outputId": "5937888d-df82-422a-8736-4148d1c64543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- transacted_at: timestamp (nullable = true)\n",
            " |-- trx_id: integer (nullable = true)\n",
            " |-- retailer_id: integer (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- amount: double (nullable = true)\n",
            " |-- city_id: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_parquet.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU49Nqs7QMF4",
        "outputId": "8279c256-8698-4141-826e-cc9756491ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----------+-----------+--------------------+-------+----------+\n",
            "|      transacted_at|    trx_id|retailer_id|         description| amount|   city_id|\n",
            "+-------------------+----------+-----------+--------------------+-------+----------+\n",
            "|2017-11-24 19:00:00|1995601912| 2077350195|Walgreen       11-25| 197.23| 216510442|\n",
            "|2017-11-24 19:00:00|1734117021|  644879053|unkn    ppd id: 7...|   8.58| 930259917|\n",
            "|2017-11-24 19:00:00|1734117022|  847200066|Wal-Mart  ppd id:...|1737.26|1646415505|\n",
            "|2017-11-24 19:00:00|1734117030| 1953761884|Home Depot     pp...|  384.5| 287177635|\n",
            "|2017-11-24 19:00:00|1734117089| 1898522855| Target        11-25|  66.33|1855530529|\n",
            "|2017-11-24 19:00:00|1734117117|  997626433|Sears  ppd id: 85...| 298.87| 957346984|\n",
            "|2017-11-24 19:00:00|1734117123| 1953761884|unkn   ppd id: 15...|  19.55|  45522086|\n",
            "|2017-11-24 19:00:00|1734117152| 1429095612|Ikea     arc id: ...|   9.39|1268541279|\n",
            "|2017-11-24 19:00:00|1734117153|  847200066|unkn        Kings...|2907.57|1483931123|\n",
            "|2017-11-24 19:00:00|1734117212| 1996661856|unkn    ppd id: 4...| 140.38| 336763936|\n",
            "|2017-11-24 19:00:00|1734117241|  486576507|              iTunes|2912.67|1663872965|\n",
            "|2017-11-24 19:00:00|2076947148|  847200066|Wal-Mart         ...|  62.83|1556600840|\n",
            "|2017-11-24 19:00:00|2076947147|  562903918|McDonald's    ccd...|  31.37| 930259917|\n",
            "|2017-11-24 19:00:00|2076947146|  511877722|unkn     ccd id: ...|1915.35|1698762556|\n",
            "|2017-11-24 19:00:00|2076947113| 1996661856|AutoZone  arc id:...| 1523.6|1759612211|\n",
            "|2017-11-24 19:00:00|2076947018|  902350112|DineEquity    arc...|  22.28|2130657559|\n",
            "|2017-11-24 19:00:00|2076946994| 1898522855|Target    ppd id:...|2589.93|2074005445|\n",
            "|2017-11-24 19:00:00|2076946985|  847200066|Wal-Mart    ppd i...|   42.2| 459344513|\n",
            "|2017-11-24 19:00:00|2076946960|  386167994|Wendy's  ppd id: ...|  14.62| 352952442|\n",
            "|2017-11-24 19:00:00|2076946954|  486576507|iTunes     ppd id...|  37.42| 485114748|\n",
            "+-------------------+----------+-----------+--------------------+-------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the whole folder with parquet files\n",
        "df_parquet = spark.read.format(\"parquet\").load(\"data/input/sales_total_parquet/*.parquet\")\n"
      ],
      "metadata": {
        "id": "4kwDCXYlRMU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read ORC Sales data\n",
        "\n",
        "df_orc = spark.read.format(\"orc\").load(\"/content/sales_data.orc\")"
      ],
      "metadata": {
        "id": "WGdpW3Z3QOKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_orc.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMDs8foKQie0",
        "outputId": "acdcba83-b60d-458d-98be-d92aa82d37e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- transacted_at: timestamp (nullable = true)\n",
            " |-- trx_id: integer (nullable = true)\n",
            " |-- retailer_id: integer (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- amount: double (nullable = true)\n",
            " |-- city_id: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_orc.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A663CKqyQlWV",
        "outputId": "f3928e62-d955-40bb-867c-9ec3d0b46764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----------+-----------+--------------------+-------+----------+\n",
            "|      transacted_at|    trx_id|retailer_id|         description| amount|   city_id|\n",
            "+-------------------+----------+-----------+--------------------+-------+----------+\n",
            "|2017-11-24 19:00:00|1995601912| 2077350195|Walgreen       11-25| 197.23| 216510442|\n",
            "|2017-11-24 19:00:00|1734117021|  644879053|unkn    ppd id: 7...|   8.58| 930259917|\n",
            "|2017-11-24 19:00:00|1734117022|  847200066|Wal-Mart  ppd id:...|1737.26|1646415505|\n",
            "|2017-11-24 19:00:00|1734117030| 1953761884|Home Depot     pp...|  384.5| 287177635|\n",
            "|2017-11-24 19:00:00|1734117089| 1898522855| Target        11-25|  66.33|1855530529|\n",
            "|2017-11-24 19:00:00|1734117117|  997626433|Sears  ppd id: 85...| 298.87| 957346984|\n",
            "|2017-11-24 19:00:00|1734117123| 1953761884|unkn   ppd id: 15...|  19.55|  45522086|\n",
            "|2017-11-24 19:00:00|1734117152| 1429095612|Ikea     arc id: ...|   9.39|1268541279|\n",
            "|2017-11-24 19:00:00|1734117153|  847200066|unkn        Kings...|2907.57|1483931123|\n",
            "|2017-11-24 19:00:00|1734117212| 1996661856|unkn    ppd id: 4...| 140.38| 336763936|\n",
            "|2017-11-24 19:00:00|1734117241|  486576507|              iTunes|2912.67|1663872965|\n",
            "|2017-11-24 19:00:00|2076947148|  847200066|Wal-Mart         ...|  62.83|1556600840|\n",
            "|2017-11-24 19:00:00|2076947147|  562903918|McDonald's    ccd...|  31.37| 930259917|\n",
            "|2017-11-24 19:00:00|2076947146|  511877722|unkn     ccd id: ...|1915.35|1698762556|\n",
            "|2017-11-24 19:00:00|2076947113| 1996661856|AutoZone  arc id:...| 1523.6|1759612211|\n",
            "|2017-11-24 19:00:00|2076947018|  902350112|DineEquity    arc...|  22.28|2130657559|\n",
            "|2017-11-24 19:00:00|2076946994| 1898522855|Target    ppd id:...|2589.93|2074005445|\n",
            "|2017-11-24 19:00:00|2076946985|  847200066|Wal-Mart    ppd i...|   42.2| 459344513|\n",
            "|2017-11-24 19:00:00|2076946960|  386167994|Wendy's  ppd id: ...|  14.62| 352952442|\n",
            "|2017-11-24 19:00:00|2076946954|  486576507|iTunes     ppd id...|  37.42| 485114748|\n",
            "+-------------------+----------+-----------+--------------------+-------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the whole folder with orc files\n",
        "df_parquet = spark.read.format(\"orc\").load(\"data/input/sales_total_orc/*.orc\")\n"
      ],
      "metadata": {
        "id": "LtDGamHvQnFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Benefits of Columnar Storage\n",
        "\n",
        "# Lets create a simple Python decorator - {get_time} to get the execution timings\n",
        "# If you dont know about Python decorators - check out : https://www.geeksforgeeks.org/decorators-in-python/\n",
        "import time\n",
        "\n",
        "def get_time(func):\n",
        "    def inner_get_time() -> str:\n",
        "        start_time = time.time()\n",
        "        func()\n",
        "        end_time = time.time()\n",
        "        return (f\"Execution time: {(end_time - start_time)*1000} ms\")\n",
        "    print(inner_get_time())"
      ],
      "metadata": {
        "id": "WF62pU80R6jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@get_time\n",
        "def x():\n",
        "    df = spark.read.format(\"parquet\").load(\"/content/sales_data.parquet\")\n",
        "    df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNBTTvhtSnES",
        "outputId": "63d4160a-245c-44b9-8e2e-94280e05f0b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution time: 594.5262908935547 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@get_time\n",
        "def x():\n",
        "    df = spark.read.format(\"parquet\").load(\"/content/sales_data.parquet\")\n",
        "    df.select(\"trx_id\").count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC0XeKxWSua8",
        "outputId": "08c4c0e7-ea9c-4fec-9e8e-3222976f3844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution time: 359.3740463256836 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BONUS TIP\n",
        "# RECURSIVE READ\n",
        "\n",
        "sales_recursive\n",
        "|__ sales_1\\1.parquet\n",
        "|__ sales_1\\sales_2\\2.parquet"
      ],
      "metadata": {
        "id": "JeZlJiaMTd0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = spark.read.format(\"parquet\").load(\"/content/sample_data/sales1/1.parquet\")\n",
        "df_1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q2nPx_0UAlR",
        "outputId": "676d0457-1ddc-4c55-a700-f359ec9e2708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----------+-----------+--------------------+------+---------+\n",
            "|      transacted_at|    trx_id|retailer_id|         description|amount|  city_id|\n",
            "+-------------------+----------+-----------+--------------------+------+---------+\n",
            "|2017-11-24 19:00:00|1734117021|  644879053|unkn    ppd id: 7...|  8.58|930259917|\n",
            "+-------------------+----------+-----------+--------------------+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = spark.read.format(\"parquet\").load(\"/content/sample_data/sales1/sales2/2.parquet\")\n",
        "df_1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAOmrA7LUBUY",
        "outputId": "05d090c5-9bf5-46d9-93d8-6d47aac43ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----------+-----------+--------------------+------+--------+\n",
            "|      transacted_at|    trx_id|retailer_id|         description|amount| city_id|\n",
            "+-------------------+----------+-----------+--------------------+------+--------+\n",
            "|2017-11-24 19:00:00|1734117123| 1953761884|unkn   ppd id: 15...| 19.55|45522086|\n",
            "+-------------------+----------+-----------+--------------------+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Both can be read together\n",
        "\n",
        "df_1 = spark.read.format(\"parquet\").option(\"recursiveFileLookup\", True).load(\"/content/sample_data/sales1\")\n",
        "df_1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcRrdaPpUEWI",
        "outputId": "ddd82545-bdcd-4007-b914-4ea8dc1a4ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----------+-----------+--------------------+------+---------+\n",
            "|      transacted_at|    trx_id|retailer_id|         description|amount|  city_id|\n",
            "+-------------------+----------+-----------+--------------------+------+---------+\n",
            "|2017-11-24 19:00:00|1734117123| 1953761884|unkn   ppd id: 15...| 19.55| 45522086|\n",
            "|2017-11-24 19:00:00|1734117021|  644879053|unkn    ppd id: 7...|  8.58|930259917|\n",
            "+-------------------+----------+-----------+--------------------+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to read JSON files? How to parse JSON data? How to flatten JSON data? What is explode function? What is from_json function ? What is to_json function ? How to write complex schema for JSON ?"
      ],
      "metadata": {
        "id": "tLyZRGhXWE15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"Reading and Parsing JSON Files/Data\")\n",
        "    .master(\"local[*]\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "58_NY8e0WLLY",
        "outputId": "4a3b160f-149a-4e47-e153-04abd34b56ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7cc3cb15fd70>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e540ebdaa4bb:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Joins and Data Partitions</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Single line JSON file\n",
        "\n",
        "df_single = spark.read.format(\"json\").load(\"/content/order_singleline.json\")"
      ],
      "metadata": {
        "id": "2W9a5F1zaNn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_single.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQTFAuDcapAW",
        "outputId": "9674e049-c17f-4b74-9f0e-8be157e3081a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- contact: array (nullable = true)\n",
            " |    |-- element: long (containsNull = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- order_line_items: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- amount: double (nullable = true)\n",
            " |    |    |-- item_id: string (nullable = true)\n",
            " |    |    |-- qty: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_single.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5Lp-Kp4aplf",
        "outputId": "9fe65dab-a5a7-4438-a325-23d0f33f63d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+--------+--------------------+\n",
            "|             contact|customer_id|order_id|    order_line_items|\n",
            "+--------------------+-----------+--------+--------------------+\n",
            "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|\n",
            "+--------------------+-----------+--------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Multiline JSON file\n",
        "\n",
        "df_multi = spark.read.format(\"json\").option(\"multiLine\", True).load(\"/content/order_multiline.json\")"
      ],
      "metadata": {
        "id": "le6ODBGWatDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_multi.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql2qngUmbSai",
        "outputId": "4d2a9f3d-676e-4bd6-ef89-89af88675553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- contact: array (nullable = true)\n",
            " |    |-- element: long (containsNull = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- order_line_items: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- amount: double (nullable = true)\n",
            " |    |    |-- item_id: string (nullable = true)\n",
            " |    |    |-- qty: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_multi.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmDSHcdXbS4W",
        "outputId": "ccd2c87d-e69f-4338-f359-73ab5e35fe18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+--------+--------------------+\n",
            "|             contact|customer_id|order_id|    order_line_items|\n",
            "+--------------------+-----------+--------+--------------------+\n",
            "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|\n",
            "+--------------------+-----------+--------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"text\").load(\"/content/order_singleline.json\")"
      ],
      "metadata": {
        "id": "eqKuzECEbUlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0GLI2bOb0f0",
        "outputId": "81e5e417-4835-472d-aaa1-b6fdfe32e6f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- value: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bNl6Slvb4wg",
        "outputId": "d65862ab-0bd5-4e51-d700-a280f38fb84a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|               value|\n",
            "+--------------------+\n",
            "|{\"order_id\":\"O101...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "293P22D0b5-I",
        "outputId": "8d1cd473-221e-4eb3-9a78-00b2ee24c1d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|value                                                                                                                                                                              |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|{\"order_id\":\"O101\",\"customer_id\":\"C001\",\"order_line_items\":[{\"item_id\":\"I001\",\"qty\":6,\"amount\":102.45},{\"item_id\":\"I003\",\"qty\":2,\"amount\":2.01}],\"contact\":[9000010000,9000010001]}|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With Schema\n",
        "\n",
        "_schema = \"customer_id string, order_id string, contact array<long>\"\n",
        "\n",
        "df_schema = spark.read.format(\"json\").schema(_schema).load(\"/content/order_singleline.json\")\n",
        "df_schema.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1L4Lay1b_SB",
        "outputId": "04a2aaa9-46fd-4e7a-8609-7ff415ef68b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+--------------------+\n",
            "|customer_id|order_id|             contact|\n",
            "+-----------+--------+--------------------+\n",
            "|       C001|    O101|[9000010000, 9000...|\n",
            "+-----------+--------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_schema1 = \"contact array<string>, customer_id string, order_id string, order_line_items array<struct<amount double, item_id string, qty long>>\"\n",
        "df_schema_new = spark.read.format(\"json\").schema(_schema1).load(\"/content/order_singleline.json\")\n",
        "df_schema_new.printSchema()\n",
        "df_schema_new.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLCnet9AVeOK",
        "outputId": "f76150a3-b799-46cc-ca40-827a6468e113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- contact: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- order_line_items: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- amount: double (nullable = true)\n",
            " |    |    |-- item_id: string (nullable = true)\n",
            " |    |    |-- qty: long (nullable = true)\n",
            "\n",
            "+--------------------+-----------+--------+--------------------+\n",
            "|             contact|customer_id|order_id|    order_line_items|\n",
            "+--------------------+-----------+--------+--------------------+\n",
            "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|\n",
            "+--------------------+-----------+--------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function from_json to read from a column\n",
        "\n",
        "_schema = \"contact array<string>, customer_id string, order_id string, order_line_items array<struct<amount double, item_id string, qty long>>\"\n",
        "\n",
        "from pyspark.sql.functions import from_json\n",
        "\n",
        "df_expanded = df.withColumn(\"parsed\", from_json(df.value, _schema))\n",
        "\n",
        "df_expanded.printSchema()\n",
        "df_expanded.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cQYKh0VWPSv",
        "outputId": "de9ebe91-43a0-4f9b-f552-8e1c7d4d39f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- value: string (nullable = true)\n",
            " |-- parsed: struct (nullable = true)\n",
            " |    |-- contact: array (nullable = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |    |-- customer_id: string (nullable = true)\n",
            " |    |-- order_id: string (nullable = true)\n",
            " |    |-- order_line_items: array (nullable = true)\n",
            " |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |-- amount: double (nullable = true)\n",
            " |    |    |    |-- item_id: string (nullable = true)\n",
            " |    |    |    |-- qty: long (nullable = true)\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|               value|              parsed|\n",
            "+--------------------+--------------------+\n",
            "|{\"order_id\":\"O101...|{[9000010000, 900...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to_json to parse a JSON string\n",
        "from pyspark.sql.functions import to_json\n",
        "\n",
        "df_unparsed = df_expanded.withColumn(\"unparsed\", to_json(df_expanded.parsed))\n",
        "df_unparsed.printSchema()\n",
        "df_unparsed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKkLVbTIegVW",
        "outputId": "2505f5a7-0ed5-447f-e1f9-4ad5f4049b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- value: string (nullable = true)\n",
            " |-- parsed: struct (nullable = true)\n",
            " |    |-- contact: array (nullable = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |    |-- customer_id: string (nullable = true)\n",
            " |    |-- order_id: string (nullable = true)\n",
            " |    |-- order_line_items: array (nullable = true)\n",
            " |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |-- amount: double (nullable = true)\n",
            " |    |    |    |-- item_id: string (nullable = true)\n",
            " |    |    |    |-- qty: long (nullable = true)\n",
            " |-- unparsed: string (nullable = true)\n",
            "\n",
            "+--------------------+--------------------+--------------------+\n",
            "|               value|              parsed|            unparsed|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|{\"order_id\":\"O101...|{[9000010000, 900...|{\"contact\":[\"9000...|\n",
            "+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_unparsed.select(\"unparsed\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU-aIsOhejXB",
        "outputId": "888e366d-d65e-413d-f9f5-638fef770fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|unparsed                                                                                                                                                                               |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|{\"contact\":[\"9000010000\",\"9000010001\"],\"customer_id\":\"C001\",\"order_id\":\"O101\",\"order_line_items\":[{\"amount\":102.45,\"item_id\":\"I001\",\"qty\":6},{\"amount\":2.01,\"item_id\":\"I003\",\"qty\":2}]}|\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get values from Parsed JSON\n",
        "df_expanded.show()\n",
        "df_1 = df_expanded.select(\"parsed.*\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMnRT-CafEn2",
        "outputId": "8d5551bc-e0b3-4ddd-93c0-1b426fce890e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|               value|              parsed|\n",
            "+--------------------+--------------------+\n",
            "|{\"order_id\":\"O101...|{[9000010000, 900...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbvCcXlIfLlm",
        "outputId": "75b4cfd9-3fc3-4199-9987-46c12ade4581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+--------+--------------------+\n",
            "|             contact|customer_id|order_id|    order_line_items|\n",
            "+--------------------+-----------+--------+--------------------+\n",
            "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|\n",
            "+--------------------+-----------+--------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "\n",
        "df_2 = df_1.withColumn(\"expanded_line_items\", explode(\"order_line_items\"))\n",
        "\n",
        "df_2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJLkJTBXfNFb",
        "outputId": "cdf29fc1-6323-4410-de2f-e0b185afcb4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+--------+--------------------+-------------------+\n",
            "|             contact|customer_id|order_id|    order_line_items|expanded_line_items|\n",
            "+--------------------+-----------+--------+--------------------+-------------------+\n",
            "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|  {102.45, I001, 6}|\n",
            "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|    {2.01, I003, 2}|\n",
            "+--------------------+-----------+--------+--------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_3 = df_2.select(\"contact\", \"customer_id\", \"order_id\", \"expanded_line_items.*\")\n",
        "df_3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i_MHWfQffoD",
        "outputId": "fd14e582-6c61-414b-b487-6ee6f7a47da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+--------+------+-------+---+\n",
            "|             contact|customer_id|order_id|amount|item_id|qty|\n",
            "+--------------------+-----------+--------+------+-------+---+\n",
            "|[9000010000, 9000...|       C001|    O101|102.45|   I001|  6|\n",
            "|[9000010000, 9000...|       C001|    O101|  2.01|   I003|  2|\n",
            "+--------------------+-----------+--------+------+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_4 = df_2.select(\"contact\", \"customer_id\", \"order_id\", \"expanded_line_items.qty\")\n",
        "df_4.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdlZ0C65fl4a",
        "outputId": "e00d3e77-fff5-48d9-8b86-bcae7887a7fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+--------+---+\n",
            "|             contact|customer_id|order_id|qty|\n",
            "+--------------------+-----------+--------+---+\n",
            "|[9000010000, 9000...|       C001|    O101|  6|\n",
            "|[9000010000, 9000...|       C001|    O101|  2|\n",
            "+--------------------+-----------+--------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explode Array fields\n",
        "df_final = df_3.withColumn(\"contact_expanded\", explode(\"contact\"))\n",
        "df_final.printSchema()\n",
        "\n",
        "df_final.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5l3HGgMf2UI",
        "outputId": "b0abfdb5-eb72-4fe5-f225-9e3e03b342e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- contact: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- amount: double (nullable = true)\n",
            " |-- item_id: string (nullable = true)\n",
            " |-- qty: long (nullable = true)\n",
            " |-- contact_expanded: string (nullable = true)\n",
            "\n",
            "+--------------------+-----------+--------+------+-------+---+----------------+\n",
            "|             contact|customer_id|order_id|amount|item_id|qty|contact_expanded|\n",
            "+--------------------+-----------+--------+------+-------+---+----------------+\n",
            "|[9000010000, 9000...|       C001|    O101|102.45|   I001|  6|      9000010000|\n",
            "|[9000010000, 9000...|       C001|    O101|102.45|   I001|  6|      9000010001|\n",
            "|[9000010000, 9000...|       C001|    O101|  2.01|   I003|  2|      9000010000|\n",
            "|[9000010000, 9000...|       C001|    O101|  2.01|   I003|  2|      9000010001|\n",
            "+--------------------+-----------+--------+------+-------+---+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.drop(\"contact\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVa-wIf4gH3M",
        "outputId": "7d69a18b-12dd-4cf4-87be-bfaab91e7ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+-------+---+----------------+\n",
            "|customer_id|order_id|amount|item_id|qty|contact_expanded|\n",
            "+-----------+--------+------+-------+---+----------------+\n",
            "|       C001|    O101|102.45|   I001|  6|      9000010000|\n",
            "|       C001|    O101|102.45|   I001|  6|      9000010001|\n",
            "|       C001|    O101|  2.01|   I003|  2|      9000010000|\n",
            "|       C001|    O101|  2.01|   I003|  2|      9000010001|\n",
            "+-----------+--------+------+-------+---+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15 How Spark Writes data | Write modes in Spark | Write data with Partition | Default Parallelism"
      ],
      "metadata": {
        "id": "KEhpfsTokrYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"Writing data\")\n",
        "    .master(\"local[*]\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "wh4ej3zZktQo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "b15fc0e7-69d4-425b-bdd0-a5f2cda05a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f94983cc140>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7ccc26836b02:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Writing data</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark available cores with defaultParallism in Spark UI\n",
        "\n",
        "spark.sparkContext.defaultParallelism"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRLZnOLd4UCx",
        "outputId": "b3e88b50-dfbd-42b9-ad18-bbc8a9bd88fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Emp Data & Schema\n",
        "\n",
        "emp_data = [\n",
        "    [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"],\n",
        "    [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"],\n",
        "    [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"],\n",
        "    [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"],\n",
        "    [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"],\n",
        "    [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"],\n",
        "    [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"],\n",
        "    [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"],\n",
        "    [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"],\n",
        "    [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"],\n",
        "    [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"],\n",
        "    [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"],\n",
        "    [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"],\n",
        "    [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"],\n",
        "    [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"],\n",
        "    [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"],\n",
        "    [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"],\n",
        "    [\"018\",\"104\",\"Nancy Liu\",\"29\",\"Female\",\"50000\",\"2017-06-01\"],\n",
        "    [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"],\n",
        "    [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"]\n",
        "]\n",
        "\n",
        "emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\""
      ],
      "metadata": {
        "id": "1zVWKa3c4WT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create emp DataFrame\n",
        "\n",
        "emp = spark.createDataFrame(data=emp_data, schema=emp_schema)"
      ],
      "metadata": {
        "id": "-0ejVecQ4pPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get number of partitions and show data\n",
        "\n",
        "emp.rdd.getNumPartitions()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCnGSiiJ4rtG",
        "outputId": "b4005696-335f-4819-f6d9-422956c50097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wvaow2cE4yz9",
        "outputId": "639a9f0c-4939-4672-84b6-52e86bbb664d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
            "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
            "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
            "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
            "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
            "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
            "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
            "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
            "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
            "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
            "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
            "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
            "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
            "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
            "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
            "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
            "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the data in parquet format\n",
        "\n",
        "emp.write.format(\"parquet\").save(\"/content/sample_data/emp.parquet\")"
      ],
      "metadata": {
        "id": "D_xPO99D41U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View data partition information\n",
        "from pyspark.sql.functions import spark_partition_id\n",
        "\n",
        "emp.withColumn(\"partition_id\", spark_partition_id()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_YHnBb25C_s",
        "outputId": "1d4b8f03-2d12-493c-9a80-9002ad1c55e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|partition_id|\n",
            "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
            "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|           0|\n",
            "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|           0|\n",
            "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|           0|\n",
            "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|           0|\n",
            "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|           0|\n",
            "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|           0|\n",
            "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|           0|\n",
            "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|           0|\n",
            "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|           0|\n",
            "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|           0|\n",
            "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|           1|\n",
            "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|           1|\n",
            "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|           1|\n",
            "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|           1|\n",
            "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|           1|\n",
            "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|           1|\n",
            "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|           1|\n",
            "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|           1|\n",
            "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|           1|\n",
            "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|           1|\n",
            "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp.write.format(\"csv\").option(\"header\", True).save(\"/content/sample_data/11/3/emp.csv\")\n"
      ],
      "metadata": {
        "id": "ZdsiR_rk5dCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the data with Partition to output location\n",
        "\n",
        "emp.write.format(\"csv\").partitionBy(\"department_id\").option(\"header\", True).save(\"/content/sample_data/11/4/emp.csv\")"
      ],
      "metadata": {
        "id": "405Tp3Oi5mJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "|employee\\_id|name|age|gender|salary|hire\\_date|\n",
        "|---|---|---|---|---|---|\n",
        "|012|Susan Chen|31|Female|54000|2017-02-15|\n",
        "|017|George Wang|34|Male|57000|2016-03-15|"
      ],
      "metadata": {
        "id": "5EIm-NXA-bhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Modes - append, overwrite, ignore and error\n",
        "\n",
        "emp.write.format(\"csv\").mode(\"append\").option(\"header\", True).save(\"data/output/11/3/emp.csv\")"
      ],
      "metadata": {
        "id": "qAE1C2TZ9j9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "created 2 more csv's"
      ],
      "metadata": {
        "id": "YMcyp5mBD3RD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp.write.format(\"csv\").mode(\"overwrite\").option(\"header\", True).save(\"data/output/11/3/emp.csv\")"
      ],
      "metadata": {
        "id": "HFptFKr_D7Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "it will clean and create the"
      ],
      "metadata": {
        "id": "tGESL9g0EKWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp.write.format(\"csv\").mode(\"ignore\").option(\"header\", True).save(\"data/output/11/3/emp.csv\")"
      ],
      "metadata": {
        "id": "NvHtuLmCYhB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "it will ignore if the data is available"
      ],
      "metadata": {
        "id": "pPxBs3AkYl99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp.write.format(\"csv\").mode(\"error\").option(\"header\", True).save(\"data/output/11/3/emp.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "38ahmkUgYr81",
        "outputId": "a5066b86-3b57-459c-8f5b-124564214c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_ALREADY_EXISTS] Path file:/content/data/output/11/3/emp.csv already exists. Set mode as \"overwrite\" to overwrite the existing path.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3620132482.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/output/11/3/emp.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minsertInto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/content/data/output/11/3/emp.csv already exists. Set mode as \"overwrite\" to overwrite the existing path."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "it will fail whne data is there"
      ],
      "metadata": {
        "id": "FwoMl3w4Yx0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bonus TIP\n",
        "# What if we need to write only 1 output file to share with DownStream?\n",
        "\n",
        "emp.repartition(1).write.format(\"csv\").option(\"header\", True).save(\"data/output/11/5/emp.csv\")\n",
        "\n",
        "# here all the 2 partition is reduced to 1 and only 1 file was create , shuffling of data happened"
      ],
      "metadata": {
        "id": "aSCyUbfTZBM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16 Understand Spark Execution on Cluster | Cluster Manager | Cluster Deployment Modes | Spark Submit"
      ],
      "metadata": {
        "id": "DXt9f3m7Z5d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"Cluster Execution\")\n",
        "    .master(\"spark://17e348267994:7077\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "ZWKbFQzFmD-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "changing the excutor to 4 from 8"
      ],
      "metadata": {
        "id": "VFJlOwwcmsv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"Cluster Execution\")\n",
        "    .master(\"spark://17e348267994:7077\")\n",
        "    .config(\"spark.executor.instances\", 4)\n",
        "    .config(\"spark.executor.cores\", 4)\n",
        "    .config(\"spark.executor.memory\", \"512M\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "aSYXdurimnjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample data frame\n",
        "\n",
        "df = spark.range(10)"
      ],
      "metadata": {
        "id": "m5z7rnpgm7LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the data of the data frame\n",
        "\n",
        "df.write.format(\"csv\").option(\"header\", True).save(\"/data/output/15/3/range.csv\")\n"
      ],
      "metadata": {
        "id": "isc3CAg1nAHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop Spark Settion\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "vt2Rn3xxnGnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'''\n",
        "##### To setup PySpark Cluster with 2 worker, 1 master and 1 history server\n",
        "Follow the below instructions carefully 👇🏻\n",
        "- Clone or Download the docker images repository from `https://github.com/subhamkharwal/docker-images`\n",
        "- Open CMD prompt (on Windows) or Terminal (on Mac) and move the cloned/downloaded folder\n",
        "- Change to folder `pyspark-cluster-with-jupyter` and run command `docker compose up`\n",
        "- The above command would setup a group of containers with 1 Jupyter Lab, 1 Master node, 2 worker nodes and 1 history server\n",
        "- Run all the containers, go into the logs of Jupyter Lab container in Docker Desktop and copy the token from the URL which looks like `http://127.0.0.1:8888/lab?token=c23436751add815d6fce10071c3958aac7b4f8ebbcf05255`\n",
        "- Open Jupyter Lab on url `https://localhost:8888`, paste the token and setup a new password.\n",
        "- [IMPORTANT] Make sure to place your file to read or write your files to location `/data/<your path>` in order to work with cluster. (/data is important, this path is mounted across the cluster to access files or data)\n",
        "- To see all your data files after execution, run the below command\n",
        "```shell\n",
        "%%sh\n",
        "ls -ltr /data/\n",
        "```\n",
        "'''\n",
        "\n",
        "# Spark Session\n",
        "\n",
        "# file name - 12_understand_cluster.py\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"Cluster Execution\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "df = spark.range(10)\n",
        "\n",
        "df.write.format(\"csv\").option(\"header\", True).save(\"/data/output/15/6/range.csv\")"
      ],
      "metadata": {
        "id": "GijthbH5niOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#spark submit"
      ],
      "metadata": {
        "id": "UlXj4iLtpSuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "root@4c1bcb0312:/spark# ./bin/spark-submit --master spark://17e345857838373:7077 --num-executors 3 --executor-cores 2 --executor-memory 512M /home/jupyter/pyspark-zero-to-hero/12_understand_cluster.py"
      ],
      "metadata": {
        "id": "RHj3nOYFn-c_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17 User Defined Function (UDF) | How Spark works with UDF | How to register Python UDF"
      ],
      "metadata": {
        "id": "1bL01XFEpYAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "UDF (User-Defined Function)\n",
        "→ A UDF is a custom function that you define in Python/Scala/SQL and register with Spark so that you can use it on Spark DataFrame columns.\n",
        "\n",
        "In simple words:\n",
        "\n",
        "⭐ A UDF lets you apply your own custom logic that Spark does NOT provide natively."
      ],
      "metadata": {
        "id": "q97AzI3XI6ap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⭐ Interview One-Liner\n",
        "\n",
        "A UDF in Spark is a User-Defined Function that lets you apply custom logic on DataFrame columns when built-in Spark functions are not enough. However, UDFs are slower and should be used only when necessary."
      ],
      "metadata": {
        "id": "pmDevM-pI7k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"User Defined Functions\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.executor.cores\", 2)\n",
        "    .config(\"spark.cores.max\", 6)\n",
        "    .config(\"spark.executor.memory\", \"512M\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "cA_9SBt8pa2n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "32487d9b-6b71-4403-9d70-94026c0dba22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e33014fa330>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://bb7bf50bcb68:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>User Defined Functions</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read employee data\n",
        "\n",
        "emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\"\n",
        "\n",
        "emp = spark.read.format(\"csv\").option(\"header\", True).schema(emp_schema).load(\"/content/emp.txt\")\n",
        "\n",
        "emp.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2m3H0LFOV2F",
        "outputId": "aafa874e-b28d-4488-a0a3-f5819db606d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwuEPclWQ3rY",
        "outputId": "e6770497-38d6-4bcd-b3b3-1466c5a1da8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
            "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
            "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
            "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
            "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
            "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
            "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
            "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
            "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
            "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
            "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
            "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
            "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
            "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
            "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
            "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
            "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to generate 10% of Salary as Bonus\n",
        "\n",
        "def bonus(salary):\n",
        "    return int(salary) * 0.1"
      ],
      "metadata": {
        "id": "JQD6zAa5RH9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "\n",
        "bonus_udf = udf(bonus) # it can we used fro pyspark API\n",
        "\n",
        "# Here spark will create a python process to generate the Data\n",
        "# then selarialization and de-serialization works"
      ],
      "metadata": {
        "id": "ui9FCQ01SvBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emp.withColumn(\"bonus\", bonus_udf(\"salary\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQzibvAdSxoF",
        "outputId": "a7f9037c-0323-41e3-f39c-3a910bf91632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date| bonus|\n",
            "+-----------+-------------+-------------+---+------+------+----------+------+\n",
            "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|5000.0|\n",
            "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|4500.0|\n",
            "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|5500.0|\n",
            "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|4800.0|\n",
            "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|6000.0|\n",
            "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|5200.0|\n",
            "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|7000.0|\n",
            "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|5100.0|\n",
            "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|5800.0|\n",
            "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|4700.0|\n",
            "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|6500.0|\n",
            "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|5400.0|\n",
            "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|7500.0|\n",
            "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|4600.0|\n",
            "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|6300.0|\n",
            "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|4900.0|\n",
            "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|5700.0|\n",
            "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|5000.0|\n",
            "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|6200.0|\n",
            "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|5300.0|\n",
            "+-----------+-------------+-------------+---+------+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Register as UDF\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "bonus_udf = udf(bonus)\n",
        "\n",
        "spark.udf.register(\"bonus_sql_udf\", bonus, \"double\")# so that we can use it for Spark SQL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "kj5W0RFlRO4X",
        "outputId": "dd156770-c2cb-4fe2-e48e-47360f9b954a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.bonus(salary)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>bonus</b><br/>def bonus(salary)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/tmp/ipython-input-3173972359.py</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new column as bonus using UDF\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "emp.withColumn(\"bonus\", expr(\"bonus_sql_udf(salary)\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg34wqBzRX27",
        "outputId": "0a744f3b-28d1-40cc-cb0a-76f816971c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date| bonus|\n",
            "+-----------+-------------+-------------+---+------+------+----------+------+\n",
            "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|5000.0|\n",
            "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|4500.0|\n",
            "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|5500.0|\n",
            "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|4800.0|\n",
            "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|6000.0|\n",
            "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|5200.0|\n",
            "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|7000.0|\n",
            "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|5100.0|\n",
            "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|5800.0|\n",
            "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|4700.0|\n",
            "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|6500.0|\n",
            "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|5400.0|\n",
            "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|7500.0|\n",
            "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|4600.0|\n",
            "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|6300.0|\n",
            "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|4900.0|\n",
            "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|5700.0|\n",
            "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|5000.0|\n",
            "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|6200.0|\n",
            "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|5300.0|\n",
            "+-----------+-------------+-------------+---+------+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new column as bonus without UDF\n",
        "# this is happening IN JVM\n",
        "\n",
        "emp.withColumn(\"bonus\", expr(\"salary * 0.1\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Is72L3XsSK2r",
        "outputId": "0d02c682-1b8e-47a8-e12d-7041855fcc08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date| bonus|\n",
            "+-----------+-------------+-------------+---+------+------+----------+------+\n",
            "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|5000.0|\n",
            "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|4500.0|\n",
            "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|5500.0|\n",
            "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|4800.0|\n",
            "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|6000.0|\n",
            "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|5200.0|\n",
            "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|7000.0|\n",
            "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|5100.0|\n",
            "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|5800.0|\n",
            "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|4700.0|\n",
            "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|6500.0|\n",
            "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|5400.0|\n",
            "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|7500.0|\n",
            "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|4600.0|\n",
            "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|6300.0|\n",
            "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|4900.0|\n",
            "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|5700.0|\n",
            "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|5000.0|\n",
            "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|6200.0|\n",
            "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|5300.0|\n",
            "+-----------+-------------+-------------+---+------+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18 Understand DAG, Explain Plans & Spark Shuffle with Tasks |Skipped Stage |Benefit of Shuffle Write"
      ],
      "metadata": {
        "id": "SkV_ZjggiCyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"Understand Plans and DAG\")\n",
        "    .master(\"local[*]\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "Oa-kPS-liHVU",
        "outputId": "2779988a-cea2-4da0-db9e-e4536f607595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e33014fa330>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://bb7bf50bcb68:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>User Defined Functions</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable AQE and Broadcast join\n",
        "\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
      ],
      "metadata": {
        "id": "0atvn40wijSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check default Parallism\n",
        "\n",
        "spark.sparkContext.defaultParallelism"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt_eV8QUikT1",
        "outputId": "005c8da1-8aff-445f-a8c6-95657ddfe02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataframes\n",
        "\n",
        "df_1 = spark.range(4, 200, 2)\n",
        "df_2 = spark.range(2, 200, 4)"
      ],
      "metadata": {
        "id": "Z64uyIKhipft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2.rdd.getNumPartitions()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQdxaTJpizF4",
        "outputId": "f5e1ccd0-cdc4-4a7b-f76a-cb891a495037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-partition data\n",
        "\n",
        "df_3 = df_1.repartition(5)\n",
        "df_4 = df_2.repartition(7)"
      ],
      "metadata": {
        "id": "7Wgq-Fbfi1UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_4.rdd.getNumPartitions()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RIWdJFpi5oQ",
        "outputId": "bafafc70-02c8-4ca6-c78e-4d8d7e08b830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the dataframes\n",
        "# default shuffle partition is 200\n",
        "\n",
        "df_joined = df_3.join(df_4, on=\"id\")"
      ],
      "metadata": {
        "id": "9Ch9Knj767k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the sum of ids\n",
        "\n",
        "df_sum = df_joined.selectExpr(\"sum(id) as total_sum\")"
      ],
      "metadata": {
        "id": "eP_pEAUe77ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# View data\n",
        "df_sum.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oFwlPJx7-_S",
        "outputId": "cdef26fa-74e0-46fc-ae53-9f955504c2bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|total_sum|\n",
            "+---------+\n",
            "|     4998|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sum.explain()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_fIqCSB8XmP",
        "outputId": "64965d9e-8b5c-4a8f-e83b-70f65942a2f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "*(6) HashAggregate(keys=[], functions=[sum(id#217L)])\n",
            "+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=261]\n",
            "   +- *(5) HashAggregate(keys=[], functions=[partial_sum(id#217L)])\n",
            "      +- *(5) Project [id#217L]\n",
            "         +- *(5) SortMergeJoin [id#217L], [id#219L], Inner\n",
            "            :- *(2) Sort [id#217L ASC NULLS FIRST], false, 0\n",
            "            :  +- Exchange hashpartitioning(id#217L, 200), ENSURE_REQUIREMENTS, [plan_id=245]\n",
            "            :     +- Exchange RoundRobinPartitioning(5), REPARTITION_BY_NUM, [plan_id=244]\n",
            "            :        +- *(1) Range (4, 200, step=2, splits=2)\n",
            "            +- *(4) Sort [id#219L ASC NULLS FIRST], false, 0\n",
            "               +- Exchange hashpartitioning(id#219L, 200), ENSURE_REQUIREMENTS, [plan_id=252]\n",
            "                  +- Exchange RoundRobinPartitioning(7), REPARTITION_BY_NUM, [plan_id=251]\n",
            "                     +- *(3) Range (2, 200, step=4, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Union the data again to see the skipped stages\n",
        "\n",
        "df_union = df_sum.union(df_4)"
      ],
      "metadata": {
        "id": "Zw7fcp1J9Ez_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_union.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj5QAHvl9Iqc",
        "outputId": "495edb56-5f3b-40e8-9b58-0b54a232ff95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|total_sum|\n",
            "+---------+\n",
            "|     4998|\n",
            "|       14|\n",
            "|       86|\n",
            "|       42|\n",
            "|      146|\n",
            "|      134|\n",
            "|      142|\n",
            "|      162|\n",
            "|       74|\n",
            "|       94|\n",
            "|       34|\n",
            "|      198|\n",
            "|      182|\n",
            "|      126|\n",
            "|      174|\n",
            "|       98|\n",
            "|       10|\n",
            "|       82|\n",
            "|      122|\n",
            "|      186|\n",
            "+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explain plan\n",
        "\n",
        "df_union.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xr71csS9LmU",
        "outputId": "af6710f6-a1c3-4955-fb44-fe0ecb909517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "Union\n",
            ":- *(6) HashAggregate(keys=[], functions=[sum(id#217L)])\n",
            ":  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=499]\n",
            ":     +- *(5) HashAggregate(keys=[], functions=[partial_sum(id#217L)])\n",
            ":        +- *(5) Project [id#217L]\n",
            ":           +- *(5) SortMergeJoin [id#217L], [id#219L], Inner\n",
            ":              :- *(2) Sort [id#217L ASC NULLS FIRST], false, 0\n",
            ":              :  +- Exchange hashpartitioning(id#217L, 200), ENSURE_REQUIREMENTS, [plan_id=483]\n",
            ":              :     +- Exchange RoundRobinPartitioning(5), REPARTITION_BY_NUM, [plan_id=482]\n",
            ":              :        +- *(1) Range (4, 200, step=2, splits=2)\n",
            ":              +- *(4) Sort [id#219L ASC NULLS FIRST], false, 0\n",
            ":                 +- Exchange hashpartitioning(id#219L, 200), ENSURE_REQUIREMENTS, [plan_id=490]\n",
            ":                    +- Exchange RoundRobinPartitioning(7), REPARTITION_BY_NUM, [plan_id=489]\n",
            ":                       +- *(3) Range (2, 200, step=4, splits=2)\n",
            "+- ReusedExchange [id#238L], Exchange RoundRobinPartitioning(7), REPARTITION_BY_NUM, [plan_id=489]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we knowspark optimizes its Dag by skiping , In shuffling or excahnge data - spark will write the data and next step it will read the data\n",
        "\n",
        "The benefit is - if it fails in subsequent or next stage , it can read the data from last shuffle and does not have to trigger the stage"
      ],
      "metadata": {
        "id": "-_QHatZL9z0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame to RDD\n",
        "\n",
        "df_1.rdd\n",
        "\n",
        "# any dataframe that you see is an abstraction of RDD\n",
        "\n",
        "# RDD can be used only when we have to distribute the\n",
        "# data physically with the help of code or you have to\n",
        "# work extensively with spark core APIs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isGnM8nB-aRL",
        "outputId": "39e2de36-dfbd-4b73-fa39-f3fd485f6808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MapPartitionsRDD[112] at javaToPython at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19 Understand and Optimize Shuffle in Spark\n"
      ],
      "metadata": {
        "id": "iGqE1QRLCHSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"Optimizing Shuffles\")\n",
        "    .master(\"local[*]\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark\n",
        "\n",
        "# Spark Session\n",
        "# from pyspark.sql import SparkSession\n",
        "\n",
        "# spark = (\n",
        "#     SparkSession\n",
        "#     .builder\n",
        "#     .appName(\"Optimizing Shuffles\")\n",
        "#     .master(\"spark://17e348267994:7077\")\n",
        "#     .config(\"spark.cores.max\", 16)\n",
        "#     .config(\"spark.executor.cores\", 4)\n",
        "#     .config(\"spark.executor.memory\", \"512M\")\n",
        "#     .getOrCreate()\n",
        "# )\n",
        "\n",
        "# spark"
      ],
      "metadata": {
        "id": "MMm94_R3_Vdt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "141e9177-ba65-4ac5-9c79-2dd27820c08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7c83495f4b60>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b318c21541b0:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Optimizing Shuffles</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Spark defaultParallelism\n",
        "\n",
        "spark.sparkContext.defaultParallelism"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PvpfkwzuR0_",
        "outputId": "99fee921-090e-4c8a-9544-465066b97fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Disable AQE and Broadcast join\n",
        "\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
      ],
      "metadata": {
        "id": "-PpdwGR8u3IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read EMP CSV file with 10M records\n",
        "\n",
        "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
        "\n",
        "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/content/employee_records.csv\")"
      ],
      "metadata": {
        "id": "T0n5ri2Z9uiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find out avg salary as per dept\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "emp_avg = emp.groupBy(\"department_id\").agg(avg(\"salary\").alias(\"avg_sal\"))"
      ],
      "metadata": {
        "id": "P80ILX2F-O-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write data for performance Benchmarking\n",
        "\n",
        "emp_avg.write.format(\"noop\").mode(\"overwrite\").save()\n",
        "\n",
        "#NOOP - it will be used for benchmarking , it will not write the data and will only traverse the benchmarking - only reads the data not writing"
      ],
      "metadata": {
        "id": "ssiw5ewPBJOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Spark Shuffle Partition setting\n",
        "\n",
        "spark.conf.get(\"spark.sql.shuffle.partitions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aU0MTctTBn7s",
        "outputId": "f7bc2df9-7942-4961-d8f7-2fd5e89be72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'16'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 16)\n"
      ],
      "metadata": {
        "id": "4WLshlhsBoUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import spark_partition_id\n",
        "\n",
        "emp.withColumn(\"partition_id\", spark_partition_id()).where(\"partition_id = 0\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4MOcLKoBuka",
        "outputId": "42b29ee8-3064-4030-8e36-da7739b5e7f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+------------+\n",
            "|first_name| last_name|           job_title|       dob|               email|               phone|  salary|department_id|partition_id|\n",
            "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+------------+\n",
            "|   Richard|  Morrison|Public relations ...|1973-05-05|melissagarcia@exa...|       (699)525-4827|512653.0|            8|           0|\n",
            "|     Bobby|  Mccarthy|   Barrister's clerk|1974-04-25|   llara@example.net|  (750)846-1602x7458|999836.0|            7|           0|\n",
            "|    Dennis|    Norman|Land/geomatics su...|1990-06-24| jturner@example.net|    873.820.0518x825|131900.0|           10|           0|\n",
            "|      John|    Monroe|        Retail buyer|1968-06-16|  erik33@example.net|    820-813-0557x624|485506.0|            1|           0|\n",
            "|  Michelle|   Elliott|      Air cabin crew|1975-03-31|tiffanyjohnston@e...|       (705)900-5337|604738.0|            8|           0|\n",
            "|    Ashley|   Montoya|        Cartographer|1976-01-16|patrickalexandra@...|        211.440.5466|483339.0|            6|           0|\n",
            "| Nathaniel|     Smith|     Quality manager|1985-06-28|  lori44@example.net|        936-403-3179|419644.0|            7|           0|\n",
            "|     Faith|  Cummings|Industrial/produc...|1978-07-01| ygordon@example.org|       (889)246-5588|205939.0|            7|           0|\n",
            "|  Margaret|    Sutton|Administrator, ed...|1975-08-16| diana44@example.net|001-647-530-5036x...|671167.0|            8|           0|\n",
            "|      Mary|    Sutton|   Freight forwarder|1979-12-28|  ryan36@example.com|   422.562.7254x3159|993829.0|            7|           0|\n",
            "|      Jake|      King|       Lexicographer|1994-07-11|monica93@example.org|+1-535-652-9715x6...|702101.0|            4|           0|\n",
            "|   Heather|     Haley|         Music tutor|1981-06-01|stephanie65@examp...|   (652)815-7973x298|570960.0|            6|           0|\n",
            "|    Thomas|    Thomas|Chartered managem...|2001-07-17|pwilliams@example...|001-245-848-0028x...|339441.0|            6|           0|\n",
            "|   Leonard|   Carlson|       Art therapist|1990-10-18|gabrielmurray@exa...|          9247590563|469728.0|            8|           0|\n",
            "|      Mark|      Wood|   Market researcher|1963-10-13|nicholas76@exampl...|   311.439.1606x3342|582291.0|            4|           0|\n",
            "|    Tracey|Washington|Travel agency man...|1986-05-07|  mark07@example.com|    001-912-206-6456|146456.0|            4|           0|\n",
            "|   Rachael| Rodriguez|         Media buyer|1966-12-02|griffinmary@examp...| +1-791-344-7586x548|544732.0|            1|           0|\n",
            "|      Tara|       Liu|   Financial adviser|1998-10-12|alexandraobrien@e...|        216.696.6061|399503.0|            3|           0|\n",
            "|       Ana|    Joseph|      Retail manager|1995-01-10|  rmorse@example.org|  (726)363-7526x9965|761988.0|           10|           0|\n",
            "|   Richard|      Hall|Engineer, civil (...|1967-03-02|brandoncardenas@e...| (964)451-9007x22496|660659.0|            4|           0|\n",
            "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp.write \\\n",
        "   .mode(\"overwrite\") \\\n",
        "   .partitionBy(\"department_id\") \\\n",
        "   .parquet(\"/content/output/emp_partitioned\")\n"
      ],
      "metadata": {
        "id": "qDJoj2z9IE-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the partitioned data\n",
        "\n",
        "emp_part = spark.read.format(\"parquet\").schema(_schema).option(\"header\", True).load(\"/content/output/emp_partitioned\")"
      ],
      "metadata": {
        "id": "EnyI5qXWB7Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emp_part.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0CswkU7IQkX",
        "outputId": "ed43d317-2bd6-43fa-e96c-9cb6d4c3e819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
            "| first_name|last_name|           job_title|       dob|               email|               phone|  salary|department_id|\n",
            "+-----------+---------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
            "|       Tara|      Liu|   Financial adviser|1998-10-12|alexandraobrien@e...|        216.696.6061|399503.0|            3|\n",
            "|    Matthew|  Mueller|    Catering manager|1986-08-24|smithscott@exampl...|        322.415.4939| 74506.0|            3|\n",
            "|    Anthony|    Moses|Geophysicist/fiel...|1999-10-01|selenawilliams@ex...|       (936)736-1889|348389.0|            3|\n",
            "|    Stanley|   Berger|Clinical molecula...|1980-05-13|robert60@example.org|        725.845.0840|495918.0|            3|\n",
            "|Christopher|    White|Tourist informati...|1967-11-02|clarence47@exampl...|        979-757-4546|986858.0|            3|\n",
            "|       Mary| Marshall|Learning disabili...|1993-06-09|amyramirez@exampl...|001-971-363-8398x...|937725.0|            3|\n",
            "|      Jacob|     Owen|Forest/woodland m...|1992-03-02|parkwhitney@examp...|+1-730-850-3411x7643|690800.0|            3|\n",
            "|      Sarah|    Smith|              Banker|1963-09-07| matkins@example.com|          7136440926|356665.0|            3|\n",
            "|      Heidi|     Rush|        TEFL teacher|1974-12-26|michael38@example...|   560.794.5993x4985|721064.0|            3|\n",
            "|      Laura|     Reed|Conference centre...|1973-10-19| jason91@example.net|+1-369-648-6592x8...|715123.0|            3|\n",
            "|     Miguel|   Hayden|      Pilot, airline|1966-09-30|vanessa75@example...|001-779-415-4884x...|306715.0|            3|\n",
            "|      Kevin|   Becker|Engineer, manufac...|1996-01-05|brittanytucker@ex...|   204.238.7559x0954|368118.0|            3|\n",
            "|     George|  Barajas|Clinical scientis...|1997-08-01|reginacarroll@exa...|001-967-639-9887x598|791729.0|            3|\n",
            "|     Vickie|    Wells| Designer, jewellery|1966-09-16|bowerssandy@examp...| (771)870-8170x10878|710612.0|            3|\n",
            "|      Laura|  Buckley|Research scientis...|1985-09-08|huntkayla@example...|    374.859.9615x756|323579.0|            3|\n",
            "|     Claire|    Hogan|Therapist, nutrit...|1994-02-13|lindsayolson@exam...|   (944)412-7848x671|435159.0|            3|\n",
            "|      Kelly|    Miles|    Paediatric nurse|1982-12-20|cartermichelle@ex...|001-624-538-0749x067| 29551.0|            3|\n",
            "|     Angela|   French|   Scientist, marine|1978-06-02|  aweeks@example.com|       (556)375-9686|629067.0|            3|\n",
            "|     Nicole|   Carney|      Radio producer|1988-01-21|matthewhernandez@...|    001-790-485-9818|853969.0|            3|\n",
            "|Christopher|Fernandez|Plant breeder/gen...|1991-12-06| vbishop@example.com|        559.210.6453|376446.0|            3|\n",
            "+-----------+---------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_avg = emp_part.groupBy(\"department_id\").agg(avg(\"salary\").alias(\"avg_sal\"))\n"
      ],
      "metadata": {
        "id": "sJdfM7euIlmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emp_avg.write.format(\"noop\").mode(\"overwrite\").save()\n"
      ],
      "metadata": {
        "id": "-nV15OPdIp8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are reading the parttition data - which will be faster as the shuffle write was small"
      ],
      "metadata": {
        "id": "dbits6bXI1m9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20 Data Caching in Spark | Cache vs Persist | Spark Storage Level with Persist |Partial Data Caching\n"
      ],
      "metadata": {
        "id": "fxuv4FQhJ7DS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"Understand Caching\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.executor.memory\", \"512M\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "5wbBNHp2J9wM",
        "outputId": "530e0ff8-1b40-4f9c-8161-7761845f0523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7c83495f4b60>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b318c21541b0:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Optimizing Shuffles</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Sales CSV Data - 752MB Size ~ 7.2M Records\n",
        "\n",
        "_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
        "\n",
        "df = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/content/new_sales.csv\")"
      ],
      "metadata": {
        "id": "jorAJuCXKP4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.where(\"amount > 300\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKWYPXU_LMJI",
        "outputId": "e611c77a-66d1-49a6-b200-2ac756a0602d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+-----------+--------------------+-------+----------+\n",
            "|       transacted_at|    trx_id|retailer_id|         description| amount|   city_id|\n",
            "+--------------------+----------+-----------+--------------------+-------+----------+\n",
            "|2017-11-24T19:00:...|1734117022|  847200066|Wal-Mart  ppd id:...|1737.26|1646415505|\n",
            "|2017-11-24T19:00:...|1734117030| 1953761884|Home Depot     pp...|  384.5| 287177635|\n",
            "|2017-11-24T19:00:...|1734117153|  847200066|unkn        Kings...|2907.57|1483931123|\n",
            "|2017-11-24T19:00:...|1734117241|  486576507|              iTunes|2912.67|1663872965|\n",
            "|2017-11-24T19:00:...|2076947146|  511877722|unkn     ccd id: ...|1915.35|1698762556|\n",
            "|2017-11-24T19:00:...|2076947113| 1996661856|AutoZone  arc id:...| 1523.6|1759612211|\n",
            "|2017-11-24T19:00:...|2076946994| 1898522855|Target    ppd id:...|2589.93|2074005445|\n",
            "|2017-11-24T19:00:...|2076946121|  562903918|unkn    ccd id: 5...| 315.86|1773943669|\n",
            "|2017-11-24T19:00:...|2076946063| 1070485878|Amazon.com   arc ...| 785.27|1126623009|\n",
            "|2017-11-24T19:00:...|2076944979| 1654681099|Delhaize America ...|  303.1|1243655802|\n",
            "|2017-11-24T19:00:...|2076944941| 1157343460|unkn    ppd id: 1...|2853.33|1141716004|\n",
            "|2017-11-24T19:00:...|2076944228| 1522061472|         YUM! Brands|1737.45| 592064091|\n",
            "|2017-11-24T19:00:...|2076944195| 1070485878|unkn   ppd id: 11...|2440.55|1525790470|\n",
            "|2017-11-24T19:00:...|2076944142|  847200066|Wal-Mart  ppd id:...| 331.63|1345953582|\n",
            "|2017-11-24T19:00:...|2076944073| 2077350195|Walgreen     arc ...|  396.9|2001708947|\n",
            "|2017-11-24T19:00:...|2076943052|  103953879|Rite Aid  arc id:...| 1910.8|1998549640|\n",
            "|2017-11-24T19:00:...|2076942340|  643354906|                BJ's|  372.7| 115209716|\n",
            "|2017-11-24T19:00:...|2076942282| 1445595477|Meijer    ccd id:...|  366.9|1717498102|\n",
            "|2017-11-24T19:00:...|2076942274| 2001148981|unkn    ppd id: 2...| 333.41| 559832710|\n",
            "|2017-11-24T19:00:...|2076942246|    9225731|AT&T Wireless  pp...|  396.9| 407629665|\n",
            "+--------------------+----------+-----------+--------------------+-------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache DataFrame (cache or persist)\n",
        "#df_cache().count() # count and write method will always preferred as they will scan the whole dataset resulting proper caching\n",
        "\n",
        "# storage is Disk & memory and deserialized for Cache\n",
        "# since cache will be created and it will be running faster\n",
        "df_cache = df.where(\"amount > 100\").cache().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVTLt3FgLaYV",
        "outputId": "07a29a5d-d592-4d7a-c859-ec302539eb92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+-----------+--------------------+-------+----------+\n",
            "|       transacted_at|    trx_id|retailer_id|         description| amount|   city_id|\n",
            "+--------------------+----------+-----------+--------------------+-------+----------+\n",
            "|2017-11-24T19:00:...|1995601912| 2077350195|Walgreen       11-25| 197.23| 216510442|\n",
            "|2017-11-24T19:00:...|1734117022|  847200066|Wal-Mart  ppd id:...|1737.26|1646415505|\n",
            "|2017-11-24T19:00:...|1734117030| 1953761884|Home Depot     pp...|  384.5| 287177635|\n",
            "|2017-11-24T19:00:...|1734117117|  997626433|Sears  ppd id: 85...| 298.87| 957346984|\n",
            "|2017-11-24T19:00:...|1734117153|  847200066|unkn        Kings...|2907.57|1483931123|\n",
            "|2017-11-24T19:00:...|1734117212| 1996661856|unkn    ppd id: 4...| 140.38| 336763936|\n",
            "|2017-11-24T19:00:...|1734117241|  486576507|              iTunes|2912.67|1663872965|\n",
            "|2017-11-24T19:00:...|2076947146|  511877722|unkn     ccd id: ...|1915.35|1698762556|\n",
            "|2017-11-24T19:00:...|2076947113| 1996661856|AutoZone  arc id:...| 1523.6|1759612211|\n",
            "|2017-11-24T19:00:...|2076946994| 1898522855|Target    ppd id:...|2589.93|2074005445|\n",
            "|2017-11-24T19:00:...|2076946121|  562903918|unkn    ccd id: 5...| 315.86|1773943669|\n",
            "|2017-11-24T19:00:...|2076946063| 1070485878|Amazon.com   arc ...| 785.27|1126623009|\n",
            "|2017-11-24T19:00:...|2076945932|   87529419|Albertsons  arc i...| 284.16|1813967666|\n",
            "|2017-11-24T19:00:...|2076945195|  562903918|unkn       Harris...| 165.75| 216135201|\n",
            "|2017-11-24T19:00:...|2076945156|  562903918|McDonald's  ccd i...|  119.4| 576697624|\n",
            "|2017-11-24T19:00:...|2076945132|   83040202|GameStop    arc i...| 242.75| 870108334|\n",
            "|2017-11-24T19:00:...|2076945098| 2139149619|Trader Joe's     ...| 114.17|   7441192|\n",
            "|2017-11-24T19:00:...|2076944979| 1654681099|Delhaize America ...|  303.1|1243655802|\n",
            "|2017-11-24T19:00:...|2076944941| 1157343460|unkn    ppd id: 1...|2853.33|1141716004|\n",
            "|2017-11-24T19:00:...|2076944228| 1522061472|         YUM! Brands|1737.45| 592064091|\n",
            "+--------------------+----------+-----------+--------------------+-------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Cache\n",
        "\n",
        "df.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs9TB1qLOeeu",
        "outputId": "03523764-4038-4a18-f7b2-54bf50c58ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[transacted_at: string, trx_id: string, retailer_id: string, description: string, amount: double, city_id: string]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cache = df.cache()"
      ],
      "metadata": {
        "id": "zStLCWbHOo18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cache.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOJDJy-KOyOz",
        "outputId": "8854e1d6-76e2-4851-9e58-9abbb146af7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "374079"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.where(\"amount > 300\").show() # here also Spark is reading the date from Cache"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQRoI3ptOzvs",
        "outputId": "bda6fd7e-f693-4cf2-ffec-cb13ab8abcd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+-----------+--------------------+-------+----------+\n",
            "|       transacted_at|    trx_id|retailer_id|         description| amount|   city_id|\n",
            "+--------------------+----------+-----------+--------------------+-------+----------+\n",
            "|2017-11-24T19:00:...|1734117022|  847200066|Wal-Mart  ppd id:...|1737.26|1646415505|\n",
            "|2017-11-24T19:00:...|1734117030| 1953761884|Home Depot     pp...|  384.5| 287177635|\n",
            "|2017-11-24T19:00:...|1734117153|  847200066|unkn        Kings...|2907.57|1483931123|\n",
            "|2017-11-24T19:00:...|1734117241|  486576507|              iTunes|2912.67|1663872965|\n",
            "|2017-11-24T19:00:...|2076947146|  511877722|unkn     ccd id: ...|1915.35|1698762556|\n",
            "|2017-11-24T19:00:...|2076947113| 1996661856|AutoZone  arc id:...| 1523.6|1759612211|\n",
            "|2017-11-24T19:00:...|2076946994| 1898522855|Target    ppd id:...|2589.93|2074005445|\n",
            "|2017-11-24T19:00:...|2076946121|  562903918|unkn    ccd id: 5...| 315.86|1773943669|\n",
            "|2017-11-24T19:00:...|2076946063| 1070485878|Amazon.com   arc ...| 785.27|1126623009|\n",
            "|2017-11-24T19:00:...|2076944979| 1654681099|Delhaize America ...|  303.1|1243655802|\n",
            "|2017-11-24T19:00:...|2076944941| 1157343460|unkn    ppd id: 1...|2853.33|1141716004|\n",
            "|2017-11-24T19:00:...|2076944228| 1522061472|         YUM! Brands|1737.45| 592064091|\n",
            "|2017-11-24T19:00:...|2076944195| 1070485878|unkn   ppd id: 11...|2440.55|1525790470|\n",
            "|2017-11-24T19:00:...|2076944142|  847200066|Wal-Mart  ppd id:...| 331.63|1345953582|\n",
            "|2017-11-24T19:00:...|2076944073| 2077350195|Walgreen     arc ...|  396.9|2001708947|\n",
            "|2017-11-24T19:00:...|2076943052|  103953879|Rite Aid  arc id:...| 1910.8|1998549640|\n",
            "|2017-11-24T19:00:...|2076942340|  643354906|                BJ's|  372.7| 115209716|\n",
            "|2017-11-24T19:00:...|2076942282| 1445595477|Meijer    ccd id:...|  366.9|1717498102|\n",
            "|2017-11-24T19:00:...|2076942274| 2001148981|unkn    ppd id: 2...| 333.41| 559832710|\n",
            "|2017-11-24T19:00:...|2076942246|    9225731|AT&T Wireless  pp...|  396.9| 407629665|\n",
            "+--------------------+----------+-----------+--------------------+-------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.unpersist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD9Y6dBNP9o4",
        "outputId": "ed2e9c55-3537-4bbc-e7ce-bb29046c0c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[transacted_at: string, trx_id: string, retailer_id: string, description: string, amount: double, city_id: string]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cache = df.where(\"amount > 100\").cache()"
      ],
      "metadata": {
        "id": "D1ESuGGBQEaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cache.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQuiDRQ-QL_Z",
        "outputId": "ff2523ac-20dd-4c5f-cf1c-ab7accb14354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "132517"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.where(\"amount > 50\").show() # here it will doing dataset scan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWswGnfbQPDO",
        "outputId": "ea4ff699-0924-4bdb-d1e6-030695fd8e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+-----------+--------------------+-------+----------+\n",
            "|       transacted_at|    trx_id|retailer_id|         description| amount|   city_id|\n",
            "+--------------------+----------+-----------+--------------------+-------+----------+\n",
            "|2017-11-24T19:00:...|1995601912| 2077350195|Walgreen       11-25| 197.23| 216510442|\n",
            "|2017-11-24T19:00:...|1734117022|  847200066|Wal-Mart  ppd id:...|1737.26|1646415505|\n",
            "|2017-11-24T19:00:...|1734117030| 1953761884|Home Depot     pp...|  384.5| 287177635|\n",
            "|2017-11-24T19:00:...|1734117089| 1898522855| Target        11-25|  66.33|1855530529|\n",
            "|2017-11-24T19:00:...|1734117117|  997626433|Sears  ppd id: 85...| 298.87| 957346984|\n",
            "|2017-11-24T19:00:...|1734117153|  847200066|unkn        Kings...|2907.57|1483931123|\n",
            "|2017-11-24T19:00:...|1734117212| 1996661856|unkn    ppd id: 4...| 140.38| 336763936|\n",
            "|2017-11-24T19:00:...|1734117241|  486576507|              iTunes|2912.67|1663872965|\n",
            "|2017-11-24T19:00:...|2076947148|  847200066|Wal-Mart         ...|  62.83|1556600840|\n",
            "|2017-11-24T19:00:...|2076947146|  511877722|unkn     ccd id: ...|1915.35|1698762556|\n",
            "|2017-11-24T19:00:...|2076947113| 1996661856|AutoZone  arc id:...| 1523.6|1759612211|\n",
            "|2017-11-24T19:00:...|2076946994| 1898522855|Target    ppd id:...|2589.93|2074005445|\n",
            "|2017-11-24T19:00:...|2076946899|  644879053|unkn     ccd id: ...|  91.91|2068475652|\n",
            "|2017-11-24T19:00:...|2076946121|  562903918|unkn    ccd id: 5...| 315.86|1773943669|\n",
            "|2017-11-24T19:00:...|2076946063| 1070485878|Amazon.com   arc ...| 785.27|1126623009|\n",
            "|2017-11-24T19:00:...|2076945932|   87529419|Albertsons  arc i...| 284.16|1813967666|\n",
            "|2017-11-24T19:00:...|2076945195|  562903918|unkn       Harris...| 165.75| 216135201|\n",
            "|2017-11-24T19:00:...|2076945156|  562903918|McDonald's  ccd i...|  119.4| 576697624|\n",
            "|2017-11-24T19:00:...|2076945132|   83040202|GameStop    arc i...| 242.75| 870108334|\n",
            "|2017-11-24T19:00:...|2076945098| 2139149619|Trader Joe's     ...| 114.17|   7441192|\n",
            "+--------------------+----------+-----------+--------------------+-------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MEMORY_ONLY, MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK_2\n",
        "import pyspark\n",
        "\n",
        "df_persist = df.persist(pyspark.StorageLevel.MEMORY_ONLY)"
      ],
      "metadata": {
        "id": "S7gCM7diRXGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_persist.write.format(\"noop\").mode(\"overwrite\").save() # Memory is serialized meaning nothing in Disc\n",
        "# if spark storage setting is Memory_ONLY and data is not able to fit in memory then we will See OUT of memory error\n"
      ],
      "metadata": {
        "id": "9C1a-1bSRZe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove All the Cache\n",
        "\n",
        "spark.catalog.clearCache()"
      ],
      "metadata": {
        "id": "F6ssJr6vRb5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Cache - default storage methed is Memory and Disc and data is deserialized\n",
        "In persist - default storage methed we choose to use"
      ],
      "metadata": {
        "id": "mupwjU0oSlyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**21 Broadcast Variable and Accumulators in Spark | How to use Spark Broadcast Variables**"
      ],
      "metadata": {
        "id": "tgMsIr4TTJj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"Distributed Shared Variables\")\n",
        "    .master(\"spark://17e348267994:7077\")\n",
        "    .config(\"spark.cores.max\", 16)\n",
        "    .config(\"spark.executor.cores\", 4)\n",
        "    .config(\"spark.executor.memory\", \"512M\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "XDi1PzhDTOmx",
        "outputId": "b29e652b-b741-42d1-c584-9f02f160bcc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7c83495f4b60>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b318c21541b0:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Optimizing Shuffles</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read EMP CSV data\n",
        "\n",
        "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
        "\n",
        "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/content/employee_records.csv\")\n",
        "\n",
        "emp.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp8GV6K2TX62",
        "outputId": "b12bd256-71bf-4cbe-9462-5b0da800e448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
            "|first_name| last_name|           job_title|       dob|               email|               phone|  salary|department_id|\n",
            "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
            "|   Richard|  Morrison|Public relations ...|1973-05-05|melissagarcia@exa...|       (699)525-4827|512653.0|            8|\n",
            "|     Bobby|  Mccarthy|   Barrister's clerk|1974-04-25|   llara@example.net|  (750)846-1602x7458|999836.0|            7|\n",
            "|    Dennis|    Norman|Land/geomatics su...|1990-06-24| jturner@example.net|    873.820.0518x825|131900.0|           10|\n",
            "|      John|    Monroe|        Retail buyer|1968-06-16|  erik33@example.net|    820-813-0557x624|485506.0|            1|\n",
            "|  Michelle|   Elliott|      Air cabin crew|1975-03-31|tiffanyjohnston@e...|       (705)900-5337|604738.0|            8|\n",
            "|    Ashley|   Montoya|        Cartographer|1976-01-16|patrickalexandra@...|        211.440.5466|483339.0|            6|\n",
            "| Nathaniel|     Smith|     Quality manager|1985-06-28|  lori44@example.net|        936-403-3179|419644.0|            7|\n",
            "|     Faith|  Cummings|Industrial/produc...|1978-07-01| ygordon@example.org|       (889)246-5588|205939.0|            7|\n",
            "|  Margaret|    Sutton|Administrator, ed...|1975-08-16| diana44@example.net|001-647-530-5036x...|671167.0|            8|\n",
            "|      Mary|    Sutton|   Freight forwarder|1979-12-28|  ryan36@example.com|   422.562.7254x3159|993829.0|            7|\n",
            "|      Jake|      King|       Lexicographer|1994-07-11|monica93@example.org|+1-535-652-9715x6...|702101.0|            4|\n",
            "|   Heather|     Haley|         Music tutor|1981-06-01|stephanie65@examp...|   (652)815-7973x298|570960.0|            6|\n",
            "|    Thomas|    Thomas|Chartered managem...|2001-07-17|pwilliams@example...|001-245-848-0028x...|339441.0|            6|\n",
            "|   Leonard|   Carlson|       Art therapist|1990-10-18|gabrielmurray@exa...|          9247590563|469728.0|            8|\n",
            "|      Mark|      Wood|   Market researcher|1963-10-13|nicholas76@exampl...|   311.439.1606x3342|582291.0|            4|\n",
            "|    Tracey|Washington|Travel agency man...|1986-05-07|  mark07@example.com|    001-912-206-6456|146456.0|            4|\n",
            "|   Rachael| Rodriguez|         Media buyer|1966-12-02|griffinmary@examp...| +1-791-344-7586x548|544732.0|            1|\n",
            "|      Tara|       Liu|   Financial adviser|1998-10-12|alexandraobrien@e...|        216.696.6061|399503.0|            3|\n",
            "|       Ana|    Joseph|      Retail manager|1995-01-10|  rmorse@example.org|  (726)363-7526x9965|761988.0|           10|\n",
            "|   Richard|      Hall|Engineer, civil (...|1967-03-02|brandoncardenas@e...| (964)451-9007x22496|660659.0|            4|\n",
            "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable (Lookup)\n",
        "dept_names = {1 : 'Department 1',\n",
        "              2 : 'Department 2',\n",
        "              3 : 'Department 3',\n",
        "              4 : 'Department 4',\n",
        "              5 : 'Department 5',\n",
        "              6 : 'Department 6',\n",
        "              7 : 'Department 7',\n",
        "              8 : 'Department 8',\n",
        "              9 : 'Department 9',\n",
        "              10 : 'Department 10'}"
      ],
      "metadata": {
        "id": "f7XqDRwwTdC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problems\n",
        "1. If we use a dept_df and Join with Emp_df - if will have shuffle\n",
        "2. UDF can we created with a loopup variable but here variable will be serized with the task , everytime this deserialization and serialization will be happening row by row"
      ],
      "metadata": {
        "id": "U-AsAfC0Y_aR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔶 Serialization (SER) — “Convert Object → Bytes”\n",
        "\n",
        "Meaning:\n",
        "Serialization is the process of converting a data object into a sequence of bytes (a transferable/storable format).\n",
        "\n",
        "Why:\n",
        "So it can be:\n",
        "\n",
        "sent over a network\n",
        "\n",
        "saved to a file\n",
        "\n",
        "stored in memory efficiently"
      ],
      "metadata": {
        "id": "VG9nrykmaWNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔶 Deserialization (DESER) — “Convert Bytes → Object”\n",
        "\n",
        "Meaning:\n",
        "Deserialization is the process of reconstructing the original object from the byte sequence.\n",
        "\n",
        "Why:\n",
        "So a program can use the object again."
      ],
      "metadata": {
        "id": "yz7V0P9FaULq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔶 What is a Broadcast Variable in Spark?\n",
        "\n",
        "A broadcast variable is a read-only shared variable that Spark sends only once to all executors, instead of sending it again and again with every task.\n",
        "\n",
        "It is used to efficiently share small lookup data across all worker nodes.\n",
        "\n",
        "🧠 Why Is It Needed?\n",
        "\n",
        "Without broadcast variables:\n",
        "\n",
        "Spark sends the same data with every task → waste of network + memory\n",
        "\n",
        "For large jobs, this becomes slow and expensive\n",
        "\n",
        "With broadcast variables:\n",
        "\n",
        "Spark sends the data only once to each executor\n",
        "\n",
        "All tasks on that executor use the same copy\n",
        "\n",
        "Much faster and more memory-efficient"
      ],
      "metadata": {
        "id": "3L2wA5QThV0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Broadcast the variable\n",
        "\n",
        "broadcast_dept_names = spark.sparkContext.broadcast(dept_names)"
      ],
      "metadata": {
        "id": "c2bCOm4laSJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(broadcast_dept_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "ryIBKnEmhmxi",
        "outputId": "8933271e-b50a-4064-8a3f-8e39f18d4bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.broadcast.Broadcast"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.broadcast.Broadcast</b><br/>def __init__(sc: Optional[&#x27;SparkContext&#x27;]=None, value: Optional[T]=None, pickle_registry: Optional[&#x27;BroadcastPickleRegistry&#x27;]=None, path: Optional[str]=None, sock_file: Optional[BinaryIO]=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pyspark/broadcast.py</a>A broadcast variable created with :meth:`SparkContext.broadcast`.\n",
              "Access its value through :attr:`value`.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; b = spark.sparkContext.broadcast([1, 2, 3, 4, 5])\n",
              "&gt;&gt;&gt; b.value\n",
              "[1, 2, 3, 4, 5]\n",
              "&gt;&gt;&gt; spark.sparkContext.parallelize([0, 0]).flatMap(lambda x: b.value).collect()\n",
              "[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n",
              "&gt;&gt;&gt; b.unpersist()\n",
              "\n",
              "&gt;&gt;&gt; large_broadcast = spark.sparkContext.broadcast(range(10000))</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 71);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the value of the variable\n",
        "broadcast_dept_names.value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ3aDqfehjRT",
        "outputId": "1f8b4936-0474-4656-ad04-2caa57201812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'Department 1',\n",
              " 2: 'Department 2',\n",
              " 3: 'Department 3',\n",
              " 4: 'Department 4',\n",
              " 5: 'Department 5',\n",
              " 6: 'Department 6',\n",
              " 7: 'Department 7',\n",
              " 8: 'Department 8',\n",
              " 9: 'Department 9',\n",
              " 10: 'Department 10'}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create UDF to return Department name\n",
        "\n",
        "from pyspark.sql.functions import udf, col\n",
        "\n",
        "@udf\n",
        "def get_dept_names(dept_id):\n",
        "    return broadcast_dept_names.value.get(dept_id)"
      ],
      "metadata": {
        "id": "QZTo_DYIhriy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emp_final = emp.withColumn(\"dept_name\", get_dept_names(col(\"department_id\")))\n"
      ],
      "metadata": {
        "id": "nq8L7Mbgh-Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emp_final.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCFtJjxCiMhd",
        "outputId": "c8a17acd-4f69-4e88-c042-dd1eb205df9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+-------------+\n",
            "|first_name| last_name|           job_title|       dob|               email|               phone|  salary|department_id|    dept_name|\n",
            "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+-------------+\n",
            "|   Richard|  Morrison|Public relations ...|1973-05-05|melissagarcia@exa...|       (699)525-4827|512653.0|            8| Department 8|\n",
            "|     Bobby|  Mccarthy|   Barrister's clerk|1974-04-25|   llara@example.net|  (750)846-1602x7458|999836.0|            7| Department 7|\n",
            "|    Dennis|    Norman|Land/geomatics su...|1990-06-24| jturner@example.net|    873.820.0518x825|131900.0|           10|Department 10|\n",
            "|      John|    Monroe|        Retail buyer|1968-06-16|  erik33@example.net|    820-813-0557x624|485506.0|            1| Department 1|\n",
            "|  Michelle|   Elliott|      Air cabin crew|1975-03-31|tiffanyjohnston@e...|       (705)900-5337|604738.0|            8| Department 8|\n",
            "|    Ashley|   Montoya|        Cartographer|1976-01-16|patrickalexandra@...|        211.440.5466|483339.0|            6| Department 6|\n",
            "| Nathaniel|     Smith|     Quality manager|1985-06-28|  lori44@example.net|        936-403-3179|419644.0|            7| Department 7|\n",
            "|     Faith|  Cummings|Industrial/produc...|1978-07-01| ygordon@example.org|       (889)246-5588|205939.0|            7| Department 7|\n",
            "|  Margaret|    Sutton|Administrator, ed...|1975-08-16| diana44@example.net|001-647-530-5036x...|671167.0|            8| Department 8|\n",
            "|      Mary|    Sutton|   Freight forwarder|1979-12-28|  ryan36@example.com|   422.562.7254x3159|993829.0|            7| Department 7|\n",
            "|      Jake|      King|       Lexicographer|1994-07-11|monica93@example.org|+1-535-652-9715x6...|702101.0|            4| Department 4|\n",
            "|   Heather|     Haley|         Music tutor|1981-06-01|stephanie65@examp...|   (652)815-7973x298|570960.0|            6| Department 6|\n",
            "|    Thomas|    Thomas|Chartered managem...|2001-07-17|pwilliams@example...|001-245-848-0028x...|339441.0|            6| Department 6|\n",
            "|   Leonard|   Carlson|       Art therapist|1990-10-18|gabrielmurray@exa...|          9247590563|469728.0|            8| Department 8|\n",
            "|      Mark|      Wood|   Market researcher|1963-10-13|nicholas76@exampl...|   311.439.1606x3342|582291.0|            4| Department 4|\n",
            "|    Tracey|Washington|Travel agency man...|1986-05-07|  mark07@example.com|    001-912-206-6456|146456.0|            4| Department 4|\n",
            "|   Rachael| Rodriguez|         Media buyer|1966-12-02|griffinmary@examp...| +1-791-344-7586x548|544732.0|            1| Department 1|\n",
            "|      Tara|       Liu|   Financial adviser|1998-10-12|alexandraobrien@e...|        216.696.6061|399503.0|            3| Department 3|\n",
            "|       Ana|    Joseph|      Retail manager|1995-01-10|  rmorse@example.org|  (726)363-7526x9965|761988.0|           10|Department 10|\n",
            "|   Richard|      Hall|Engineer, civil (...|1967-03-02|brandoncardenas@e...| (964)451-9007x22496|660659.0|            4| Department 4|\n",
            "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total salary of Department 6\n",
        "\n",
        "from pyspark.sql.functions import sum\n",
        "\n",
        "emp.where(\"department_id = 6\").groupBy(\"department_id\").agg(sum(\"salary\").cast(\"long\")).show()\n",
        "\n",
        "# there will be shuffle here,"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGAMnD0Ni8Qd",
        "outputId": "2c8495d8-092c-4a56-b22c-084f03035f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------------------------+\n",
            "|department_id|CAST(sum(salary) AS BIGINT)|\n",
            "+-------------+---------------------------+\n",
            "|            6|                50294510721|\n",
            "+-------------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What Are Accumulators in Spark?\n",
        "\n",
        "Accumulators are variables that workers (executors) can only add to, and only the driver can read the final value.\n",
        "\n",
        "They are mostly used for:\n",
        "\n",
        "Counters (count how many rows matched a condition)\n",
        "\n",
        "Sums (add up values across workers)\n",
        "\n",
        "Debugging & monitoring"
      ],
      "metadata": {
        "id": "m8IBPdAGkpx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accumulators # distributed variables\n",
        "\n",
        "dept_sal = spark.sparkContext.accumulator(0)"
      ],
      "metadata": {
        "id": "cD5fd4CykV80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use foreach\n",
        "\n",
        "def calculate_salary(department_id, salary):\n",
        "    if department_id == 6:\n",
        "        dept_sal.add(salary)\n",
        "\n",
        "emp.foreach(lambda row : calculate_salary(row.department_id, row.salary))"
      ],
      "metadata": {
        "id": "lzlssYezkWiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View total value\n",
        "\n",
        "dept_sal.value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-pR1ud4kYaI",
        "outputId": "43c86d49-c04f-4986-b3d0-064d9af3ae6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50294510721.0"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop Spark Session\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "6nMJS7c_keAP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}